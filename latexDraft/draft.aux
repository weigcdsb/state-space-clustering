\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{unsrtnat}
\citation{Macke2011}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{intro}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Method}{1}{section.2}\protected@file@percent }
\newlabel{method}{{2}{1}{Method}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Poisson Dynamic Factor Model}{1}{subsection.2.1}\protected@file@percent }
\citation{Johnstone2009}
\citation{El-Sayyad1973}
\citation{Chan2009}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Overview of the model. A.} Briefly describe the neuroscience motivation. \textbf  {B.} The graphical model}}{2}{figure.1}\protected@file@percent }
\citation{Keeley2019}
\citation{Miller2018}
\citation{Miller2018}
\citation{Paninski2010}
\citation{Neal2000}
\citation{Blei2006}
\citation{Lopes2004}
\citation{Bhattacharya2011}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Cluster by Mixture of Finite Mixtures Model}{3}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Inference}{3}{subsection.2.3}\protected@file@percent }
\citation{Fokoue2003}
\@writefile{toc}{\contentsline {section}{\numberline {3}Simulations}{4}{section.3}\protected@file@percent }
\newlabel{sim}{{3}{4}{Simulations}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Model Global Non-linearity by Clustering}{4}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Global non-linearity by multiple clusters} \leavevmode {\color  {red}(The labels are known now. The purpose is to check MCMC do fit well.)}Simulate 10 clusters without specifying linear dynamics, with 5 neurons each. The dimension in each cluster is $p=1 (q=2)$. All the fitted posteriors are averages from iteration 500 to 1000. \textbf  {A.} The trace plot of log-likelihood per spike. \textbf  {B.} The fitted mean firing rate. \textbf  {C.} The fitted latent vectors in the first 3 clusters.\textbf  {D.} and \textbf  {E.} Select 3/4 and 1/2 of the data as training in a speckled pattern, and replicate the sampling 50 times in each case. With each training sample, fit two models: 1) 10-cluster model: fit every single model for each, with $p=1$ (true value) in each and 2) 1-cluster model: fit a single model for all neurons, with $p$ selected by cross-validation. \textbf  {D.} The held-out log-likelihood per spike decrease for less training samples. With the same training sample, 10-cluster model performs better than 1-cluster model. The selected $p$ for 1-cluster model is shown at the top. \textbf  {E.} The differences of the held-out log-likelihood per spike between 10-cluster model and 1-cluster model. The less the training data, the more significant the improvement by fitting model cluster-wise.}}{4}{figure.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Clustering}{4}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Application: Neuropixels data}{4}{section.4}\protected@file@percent }
\newlabel{app}{{4}{4}{Application: Neuropixels data}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{4}{section.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Clustering} The same simulation setting as in Figure 2, but remove the labels. \leavevmode {\color  {red}(The cluster 3 and 5 have nearly the same spiking patterns, because of the "bad luck"... May resample with another seed)}. The fitting is conducted using all and half data as training. \textbf  {A.} The trace plot of training log-likelihood per spike and number of cluster. All the posterior results are averages from iteration 500 to 1000. \textbf  {B.} The fitted firing rate. \leavevmode {\color  {red}(This is different from figure 2, because the labels are removed)} \textbf  {C.} The similarity matrix. \textbf  {D.} and \textbf  {E.} Compare the held-out (1/2 data) likelihood for 3 fittings: 1) 10-cluster model: use the true labels and fit the model in each cluster with $p=1$; 2) cluster-on model: unknown labels but turn on the clustering, with $p=1$ (CV) and 3) 1-cluster model: single population model, with $p=8$ (CV). \textbf  {D.} The trace plot of held-out log-likelihood per spike. The traces for 10-cluster and cluster-on are close, but more variation (occasional "drop-down") in cluster-on due to estimation of unknown labels. \textbf  {E.} The histograms of posterior held-out log-likelihood per spike for 3 models. 10-cluster and cluster-on models are close, and they perform better than 1-cluster model.}}{5}{figure.3}\protected@file@percent }
\bibdata{MyCollection}
\bibcite{Macke2011}{{1}{2011}{{Macke et~al.}}{{Macke, Buesing, Cunningham, Yu, Shenoy, and Sahani}}}
\bibcite{Johnstone2009}{{2}{2009}{{Johnstone and Lu}}{{}}}
\bibcite{El-Sayyad1973}{{3}{1973}{{El-Sayyad}}{{}}}
\bibcite{Chan2009}{{4}{2009}{{Chan and Vasconcelos}}{{}}}
\bibcite{Keeley2019}{{5}{2019}{{Keeley et~al.}}{{Keeley, Zoltowski, Yu, Yates, Smith, and Pillow}}}
\bibcite{Miller2018}{{6}{2018}{{Miller and Harrison}}{{}}}
\bibcite{Paninski2010}{{7}{2010}{{Paninski et~al.}}{{Paninski, Ahmadian, Ferreira, Koyama, {Rahnama Rad}, Vidne, Vogelstein, and Wu}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Application in Neuropixels data.} We first choose a one minute recordings from spontaneous, with bin size in $0.1s$ \leavevmode {\color  {red}(29.8 to 89.8)}.\textbf  {A.} The trace plots of log-likelihood per spike and number of cluster \leavevmode {\color  {red}(Maybe again overlaid by 1/2-training results?)}. All the following results are averages from iteration 500 to 1000. \textbf  {B} The fitted firing rate. \textbf  {C.} The histogram of posterior cluster number. \textbf  {D.} We then hold out 1/2 of data in the speckled pattern and fit 3 models: 1) cluster-on model: turn on clustering with $p=1$ (CV); 2) 1-cluster: single population with $p=2$ (CV) and 3) anatomy-cluster: fit models each based on 4 anatomical labels, with $p=(1,1,1,3)$ (CV). Doing clustering is the best. Anatomical labels give the "wrong" labels, in terms of the spiking pattern, which make things even worse. \textbf  {E.} The similarity matrices when fitting four 1-min epochs: 1) spontaneous 1: 29.8s to 89.8s; 2) spontaneous 2: 1001.9s to 1061.9s; 3) natural movie: 2221.7s to 2281.7s and 4) drift grating: 1890.9s to 1950.9s. All are sorted by mode of posterior cluster index within each anatomical region. The patterns found in spontaneous are robust and there are a large confusion between VISp and CA1. The confusion between VISp and CA1 also exists in natural movie, but decrease a lot. However, the overall confusions become larger. When exposing to the drift grating movie, the confusions among all four regions decrease even more.}}{6}{figure.4}\protected@file@percent }
\bibcite{Neal2000}{{8}{2000}{{Neal}}{{}}}
\bibcite{Blei2006}{{9}{2006}{{Blei and Jordan}}{{}}}
\bibcite{Lopes2004}{{10}{2004}{{Lopes and West}}{{}}}
\bibcite{Bhattacharya2011}{{11}{2011}{{Bhattacharya and Dunson}}{{}}}
\bibcite{Williams2020}{{12}{2020}{{Williams et~al.}}{{Williams, Degleris, Wang, and Linderman}}}
\bibcite{Fokoue2003}{{13}{2003}{{Fokou{\'{e}} and Titterington}}{{}}}
\bibcite{Eden2004}{{14}{2004}{{Eden et~al.}}{{Eden, Frank, Barbieri, Solo, and Brown}}}
\bibcite{RAUCH1965}{{15}{1965}{{Rauch et~al.}}{{Rauch, Tung, and Striebel}}}
\bibcite{Hoffman2011}{{16}{2011}{{Hoffman and Gelman}}{{}}}
\citation{Paninski2010}
\citation{Macke2011}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{7}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}MCMC updates}{7}{subsection.A.1}\protected@file@percent }
\citation{Eden2004}
\citation{RAUCH1965}
\citation{Hoffman2011}
\citation{Neal2000}
\citation{Miller2018}
\gdef \@abspage@last{8}
