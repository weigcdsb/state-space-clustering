
% load packages
\documentclass[]{article}
\usepackage{amsmath,amssymb}
\usepackage{bm}
\usepackage{lmodern}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{url}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{subfigure}

\title{Model-based Clustering for Neural Populations}
\author{}
\date{}
\begin{document}
\maketitle

The goal for this research is to do model-based clustering for  neural populations, by making use of features for each counting process observation. 

\section{Notations}

Assume we can observe neural activities for \(N\) neurons, with counting observation up to \(T\) steps. Therefore, the observation is a \textsl{N-by-T} matrix, \(\mathbf{Y} \in \mathbb{Z}_{\geq 0}^{N \times T}\) ,with each row represents the recording from single neuron. Denote the recording for neuron \(i\) as
\(\mathbf{y}_{i} = (y_{i1},\ldots,y_{\text{iT}})'\), \(i = 1,\ldots N.\), with the cluster index for neuron \(i\) as
\(z_{i} \in \{ 1,\ldots\}\). The number of neurons in cluster \(j\) is
\(n_{j} = \sum_{i = 1}^{N}{I(z_{i} = j)}\), and
\(\sum_{j = 1,2,\ldots}^{}n_{j} = N\). The proportion/ probability in
cluster \(z_{i}\) is \(\rho_{z_{i}}\).

\section{Clustering Wrapper}
The model-based clustering problem can be transformed into fitting the mixture model (MM). The likelihood for each cluster depends on how we model the counting observation, but fitting strategies for MM are the same for all models. Here, I choose to fit the MM by Gibbs sampler. Depending on whether the number of cluster is finite or not, there are two versions: finite mixture model (FMM) and Dirichlet process mixture model (DPMM).

\subsection{Finite Mixture Model}	
Assume the number of cluster is \(J\). The full likelihood for these \(N\) neurons is
\[L = \prod_{i = 1}^{N}{\rho_{z_{i}}f\left( \mathbf{y}_{i}|\mathbf{\Theta}_{z_{i}} \right)} = \prod_{j = 1}^{J}{\rho_{j}^{n_{j}}\left\lbrack \prod_{i:z_{i} = j}^{}{f\left( \mathbf{y}_{i}|\mathbf{\Theta}_{j} \right)} \right\rbrack}\]
, where \(\mathbf{\Theta}_{j}\) contains all parameters in cluster \(j\) defined by the specific model. Therefore, the parameters need to update are:
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	Cluster indicator: \(\left\{ z_{i} \right\}_{i = 1}^{N}\)
	\item
	Cluster proportion: \(\mathbf{\rho} = (\rho_{1},\ldots\rho_{J})'\)
	\item
	Model parameters: \(\mathbf{\Theta}_{j}\)
\end{enumerate}
The (conditional) priors for clustering-related parameters:
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	Cluster indicator \(\left\{ z_{i} \right\}_{i = 1}^{N}\):
	\(P\left( z_{i} = j \right) = \rho_{j}\)
	\item
	Cluster proportion \(\mathbf{\rho} = (\rho_{1},\ldots\rho_{J})'\):
	\[\mathbf{\rho} \sim Dir(\delta_{1},\ldots\delta_{J})\]
	, where \(\delta_{1} = \ldots = \delta_{J} = 1\)
\end{enumerate}

So, the MCMC( Gibbs sampler) iteration for FMM is:
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	Update \(\left\{ z_{i} \right\}_{i = 1}^{N}\):
	\[P\left( z_{i} = j \middle| \mathbf{y}_{i},\left\{ \mathbf{\Theta}_{j} \right\}_{j = 1}^{J} \right) \propto \rho_{j}f\left( \mathbf{y}_{i}|\mathbf{\Theta}_{j} \right)\]
	\item
	Update \(\mathbf{\rho} = \left( \rho_{1},\ldots\rho_{J} \right)^{'}\):
	\[\mathbf{\rho}|\ \left\{ \mathbf{y}_{i} \right\}_{i = 1}^{N},\ \left\{ z_{i} \right\}_{i = 1}^{N},\left\{ \mathbf{\Theta}_{j} \right\}_{j = 1}^{J} \sim Dir(\delta_{1} + n_{1},\ldots\delta_{J} + n_{J})\]
	\item
	Update \(\mathbf{\Theta}_{j}\): this is defined by the specific model. When there's no
	\(z_{i} = j\), just sample \(\mathbf{\Theta}_{j}\) from priors or by other observation-independent ways.
\end{enumerate}

\subsection{Dirichlet Process Mixture Model}

Since calculation of posterior predictive distribution can be hard or even impossible for complicated models, instead of using the popular CRP representation of DP (Neal, 2020), I choose to use the slice sampler (\href{https://www.tandfonline.com/doi/full/10.1080/03610910601096262}{Walker, 2007}).

Use the ''stick-breaking'' construction for cluster proportion, i.e.
\[\rho_{1} = \eta_{1}\]
\[\rho_{j} = \left( 1 - \eta_{1} \right) \cdot \ldots \cdot \left( 1 - \eta_{j - 1} \right)\eta_{j}\]
\[\eta_{j} \sim Beta(1,\alpha)\]

In the slice sampler for DPMM, the parameters need to update are:
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	``stick-breaking'' elements: \(\eta_{j}\)
	\item
	Augment latent variable: \(\left\{ u_{i} \right\}_{i = 1}^{N}\)
	\item
	Model parameters: \(\mathbf{\Theta}_{j}\)
	\item
	Cluster indicator: \(\left\{ z_{i} \right\}_{i = 1}^{N}\)
\end{enumerate}

So, the MCMC( Gibbs sampler) iteration for DPMM is:
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	update \(\eta_{j}\), for
	\(j = 1,\ldots,{{z^{*} = max}\left\{ z_{i} \right\}}_{i = 1}^{N}\) as
	\[\eta_{j}|\left\{ z_{i} \right\}_{i = 1}^{N},\ldots \sim Beta(n_{j} + 1,\ N - \sum_{l = 1}^{j}n_{l} + \alpha)\]
	\item
	update \(\left\{ u_{i} \right\}_{i = 1}^{N}\):
	\[u_{i}|\mathbf{\rho,\ldots} \sim U(0,\ \rho_{z_{i}})\]
	\item
	update \(\eta_{j}\), for \(j = z^{*} + 1,\ldots,\ s^{*}\). \(s^{*}\)
	is the smallest value, s.t.
	\(\sum_{j = 1}^{s^{*}}\rho_{j} > 1 - \min{\{ u_{1},\ldots,u_{N}\}}\)
	\[\eta_{j} \sim Beta(1,\alpha)\]
	\item
	Update \(\mathbf{\Theta}_{j}\): this is defined by the specific model. When there's no
	\(z_{i} = j\), just sample \(\mathbf{\Theta}_{j}\) from priors or by other observation-independent ways.
	\item
	Update \(\left\{ z_{i} \right\}_{i = 1}^{N}\)
	\[P\left( z_{i} = j \middle| \mathbf{y}_{i},\left\{ \mathbf{\Theta}_{j} \right\},\mathbf{\rho,}\left\{ u_{i} \right\}_{i = 1}^{N} \right) = \frac{f\left( \mathbf{y}_{i}|\mathbf{\Theta}_{j} \right)}{\sum_{j:\rho_{j} > u_{i}}^{}{f\left( \mathbf{y}_{i}|\mathbf{\Theta}_{j} \right)}}\]
\end{enumerate}

\section{linear Dynamical System Model}
Here, I model the observations by a linear dynamical system (LDS) model.

LDS models the multi-dimensional time series using a lower dimensional latent representation of the system, which evolves over time according to linear dynamics. By specifying the linear dynamics and process noise covariance, we can also handle the interactions between different neural populations (clusters).

\subsection{Model Details}
Denote the latent vector in cluster \(j\) as
\(\mathbf{x}_{t}^{(j)} \in \mathbb{R}^{p_{j}}\). For simplicity, assume all \(p_{j} = p\). Each observation follows a Poisson distribution:
\[\log\lambda_{\text{it}} = d_{i} + \mathbf{c'}_{i}\mathbf{x}_{t}^{(z_{i})}\]
\[y_{\text{it}} \sim Poisson(\lambda_{\text{it}})\]
, where \(\mathbf{c}_{i} \in \mathbb{R}^{p}\) and
\(\mathbf{x}_{t}^{(z_{i})} \in \mathbb{R}^{p}\).

Although the loading (\(d_{i}\) and \(\mathbf{c}_{i}\)) is determined by neuron index \(i\), the distribution is also cluster-dependent. That is,
\[\left( d_{i},\mathbf{c}'_{i} \right)' \sim N(\bm{\mu}_{\text{dc}}^{\left( z_{i} \right)},\mathbf{\Sigma}_{\text{dc}}^{(z_{i})})\]
By doing this, the loading within each cluster is also correlated.

Denote all latent states as \(\mathbf{x}_{t} = \left( {\mathbf{x'}_{t}^{(1)}},{\mathbf{x'}_{t}^{(2)}},\ldots \right)'\) and they evolve linearly with a Gaussian noise:
\[\mathbf{x}_{1} \sim N(\mathbf{x}_{0},\mathbf{Q}_{0})\]
\[\mathbf{x}_{t + 1}|\mathbf{x}_{t} \sim N(\mathbf{A}\mathbf{x}_{t} + \mathbf{b},\mathbf{Q})\]
For simplicity, assume \(\mathbf{Q}_{0}\) is known (e.g.
\(\mathbf{Q}_{0} = \mathbf{I} \times 10^{-2}\)).

If we assume process noise covariance is block diagonal (\href{https://papers.nips.cc/paper/2020/hash/aa1f5f73327ba40d47ebce155e785aaf-Abstract.html}{Joshua et al., 2020}), we can write things as:
\[\mathbf{x}_{t + 1}^{(j)}|\mathbf{x}_{t}^{(1)},\mathbf{x}_{t}^{(2)},\ldots \sim N(\sum_{l = 1,\ldots}^{}\mathbf{A}_{j \leftarrow l}\mathbf{x}_{t}^{(l)} + \mathbf{b}_{j},\mathbf{Q}^{(j)})\]

Notice \(\left\{ \mathbf{A}_{j \leftarrow l} \right\}\) forms the full transition matrix as:
\[\mathbf{A} = \ \begin{pmatrix}
	\mathbf{A}_{1 \leftarrow 1} & \mathbf{A}_{1 \leftarrow 2} & \ldots \\
	\mathbf{A}_{2 \leftarrow 1} & \mathbf{A}_{2 \leftarrow 2} & \ldots \\
	\ldots\  & \ldots & \ldots \\
\end{pmatrix}\]
Denote the \(j^{\text{th}}\) row block of \(\mathbf{A}\) as
\(\mathbf{A}_{j} = \begin{pmatrix}
	\mathbf{A}_{j \leftarrow 1} & \mathbf{A}_{j \leftarrow 2} & \ldots \\
\end{pmatrix}\). Then,
\(\sum_{l = 1,\ldots}^{}\mathbf{A}_{j \leftarrow l}\mathbf{x}_{t}^{(l)} + \mathbf{b}_{j}\mathbf{=}\mathbf{A}_{j}\mathbf{x}_{t} + \mathbf{b}_{j}\).

If we further let \(\mathbf{Q}\) be diagonal, with the
\(k^{\text{th}}\) row of \(\mathbf{x}_{t}\), \(\mathbf{A}\),
\(\mathbf{b}\) denoted as \(x_{\text{kt}}\), \(\mathbf{a}_{k}\), \(b_{k}\). The corresponding process noise variance is \(q_{k}\). Then:
\[x_{k,t + 1}|x_{\text{kt}} \sim N\left( \mathbf{a}_{k}^{'}\mathbf{x}_{t} + b_{k},q_{k} \right)\]

To facilitate derivation in Gibbs sampler, write the linear dynamics of
\(\mathbf{x}_{t}\) in MV-GLM form, i.e.

\[\mathbf{x}_{t + 1}^{'} = \begin{pmatrix}
	1 & \mathbf{x}_{t}^{'} \\
\end{pmatrix}\begin{pmatrix}
	\mathbf{b}^{'} \\
	\mathbf{A}^{'} \\
\end{pmatrix} + \mathbf{\epsilon}_{i}^{'}\]

, where \(\mathbf{\epsilon} \sim N(0,\mathbf{Q})\). Then if we stack the
latent by row, the problem is reduced to a multivariate general linear model (MV-GLM) problem:

\[\begin{pmatrix}
	\mathbf{x}_{2}^{'} \\
	\mathbf{x}_{3}^{'} \\
	\vdots \\
	\mathbf{x}_{T}^{'} \\
\end{pmatrix} = \begin{pmatrix}
	1 & \mathbf{x}_{1}^{'} \\
	1 & \mathbf{x}_{2}^{'} \\
	\vdots & \vdots \\
	1 & \mathbf{x}_{T - 1}^{'} \\
\end{pmatrix}\begin{pmatrix}
	\mathbf{b}^{'} \\
	\mathbf{A}^{'} \\
\end{pmatrix} + \mathbf{E}\]

, where
\(\mathbf{E} = \left( \mathbf{\epsilon}_{1},\ldots,\mathbf{\epsilon}_{T - 1} \right)^{'}\).

In summary, Let
\(\mathbf{\Theta} = \left\{ \mathbf{z},\ \mathbf{d},\mathbf{C},\left\{ \mathbf{x}_{t} \right\}_{t = 1}^{T},\mathbf{x}_{0},\mathbf{Q}_{0},\mathbf{A},\ \mathbf{b},\mathbf{Q}\  \right\}\)
be the set of parameters for LDS. The number of observation/ neuron is
\(N\), and they can group into \(J\) clusters. In each cluster, there
are \(p\) latent state vectors. The recording length is \(T\).
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	\(\mathbf{z} \in \mathbb{Z}^{N}\): cluster index for each neuron/ observation,
	with \(z_{i} \in \{ 1,\ldots,J\}\) for \(i = 1,\ldots,N\).
	\item
	\(\mathbf{d} \in \mathbb{R}^{N}\) and \(\mathbf{C} \in \mathbb{R}^{N \times p}\): baseline
	\& loading of the latent
	\item
	\(\mathbf{x}_{t} \in \mathbb{R}^{Jp}\): all latent at \(t\).
	\item
	\(\mathbf{x}_{t}^{(j)} \in \mathbb{R}^{p}\) means the latent in cluster \(j\) at
	time \(t\).
	\item
	\(\mathbf{x}_{0} \in \mathbb{R}^{Jp}\) and
	\(\mathbf{Q}_{0} \in \mathbb{R}^{Jp \times Jp}\): mean and covariance of
	\(\mathbf{x}_{1}\). For simplicity, assume \(\mathbf{Q}_{0}\) is known.
	\item
	\(\mathbf{A} \in \mathbb{R}^{Jp \times Jp},\ \mathbf{b} \in \mathbb{R}^{Jp},\mathbf{Q} \in \mathbb{R}^{Jp \times Jp}\):
	linear dynamics of the latent.
\end{enumerate}

Since the progress noise is independent at each step and the observation is assumed conditional independent, the likelihood is:
\[ f(\mathbf{y}|\mathbf{\Theta})= \prod_{i = 1}^{N}\prod_{t = 1}^{T}{P(y_{it}|\mathbf{\Theta})} = \prod_{i = 1}^{N}\prod_{t = 1}^{T}{POI(y_{it}|\exp{(d_{i} + \mathbf{c'}_{i}\mathbf{x}_{t}^{(z_{i})})})}
\]
, where \(POI(\cdot|\lambda)\) is the density of \(Poisson(\lambda)\).

\textbf{DETOUR}:\\
Maybe in the future, we may switch to EM. To help with this, I also give the likelihood for complete observation \((\mathbf{y}, \left\{ \mathbf{x}_{t} \right\}_{t = 1}^{T})\). Let \(\mathbf{\Theta}'=\mathbf{\Theta}\backslash \left\{ \mathbf{x}_{t} \right\}_{t = 1}^{T}\).
\[ f(\mathbf{y}, \left\{ \mathbf{x}_{t} \right\}_{t = 1}^{T}|\mathbf{\Theta}')= \prod_{i = 1}^{N}\prod_{t = 1}^{T}{P(y_{it}, \mathbf{x}_{t}|\mathbf{\Theta}')} = \prod_{i = 1}^{N}\prod_{t = 1}^{T}{POI(y_{it}|\exp{(d_{i} + \mathbf{c'}_{i}\mathbf{x}_{t}^{(z_{i})})})}\cdot N(\mathbf{x}_{t}^{(z_{i})} |\mathbf{A}_{z_i}\mathbf{x}_{t} + \mathbf{b}_{z_i}, \mathbf{Q}_{z_i})
\]
, where \(\mathbf{A}_j\in \mathbb{R}^{p \times Jp}\) and \(\mathbf{b}_j\in \mathbb{R}^p\)  are the \(j^{\text{th}}\) row block of \(\mathbf{A}\) and \(\mathbf{b}\). \(\mathbf{Q}_j \in \mathbb{R}^{p \times p}\) is the \(j^{\text{th}}\) row and column block of \(\mathbf{Q}\).

\subsection{Conditional Priors for Parameters}
The parameters need to estimate:
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	Latent vectors: \(\left\{ \mathbf{x}_{t} \right\}_{t=1}^T\)
	\item
	Initials: \(\mathbf{x}_{0}\)
	\item
	Linear mapping (loading) for latent vectors:
	\(\left\{ d_{i} \right\}_{i = 1}^{N}\) and
	\(\left\{ \mathbf{c}_{i} \right\}_{i = 1}^{N}\)
	\item
	Mean and covariance for loading in each cluster:
	\(\left\{ \bm{\mu}_{\text{dc}}^{(j)} \right\}_{j}\) and
	\(\left\{ \mathbf{\Sigma}_{\text{dc}}^{(j)} \right\}_{j}\)
	\item
	Linear dynamics for latent vectors: \(\mathbf{A}\) and \(\mathbf{b}\)
	\item
	Process noise: \(\mathbf{Q}\)
\end{enumerate}

The conditional priors for these parameters:
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	Latent vectors \(\left\{ \mathbf{x}_{t} \right\}_{t=1}^T\): the conditional prior is
	defined by
	\[\mathbf{x}_{1} \sim N(\mathbf{x}_{0},\mathbf{Q}_{0})\]
	\[\mathbf{x}_{t + 1}|\mathbf{x}_{t} \sim N(\mathbf{A}\mathbf{x}_{t} + \mathbf{b},\mathbf{Q})\]
	\item
	Initials \(\mathbf{x}_{0}\): assume there are \(J\) clusters,
	\[\mathbf{x}_{0} \sim N(\bm{\mu}_{\mathbf{x}_{00}},\ \mathbf{\Sigma}_{\mathbf{x}_{00}})\]
	, where \(\bm{\mu}_{\mathbf{x}_{00}} = \mathbf{0}_{Jp}\) and \(\mathbf{\Sigma}_{\mathbf{x}_{00}} = \mathbf{I}_{Jp}\)
	\item
	Linear mapping (loading) for latent vectors
	\(\left\{ d_{i} \right\}_{i = 1}^{N}\) and
	\(\left\{ \mathbf{c}_{i} \right\}_{i = 1}^{N}\):
	\[\left( d_{i},\mathbf{c}'_{i} \right)' \sim N(\bm{\mu}_{\text{dc}}^{\left( z_{i} \right)},\mathbf{\Sigma}_{\text{dc}}^{(z_{i})})\]
	\item
	Mean and covariance for loading in each cluster
	Mean and covariance for loading in each cluster
	\(\left\{ \bm{\mu}_{\text{dc}}^{(j)} \right\}_{j}\) and
	\(\left\{ \mathbf{\Sigma}_{\text{dc}}^{(j)} \right\}_{j}\):
	\[\bm{\mu}_{\text{dc}}^{(j)} \sim N(\mathbf{\delta}_{dc0},\mathbf{T}_{dc0})\]
	, where \(\bm{\delta}_{dc0} = \mathbf{0}_{p + 1}\) and
	\(\mathbf{T}_{dc0} = \mathbf{I}_{p + 1}\)
	
	\[\mathbf{\Sigma}_{\text{dc}}^{(j)} \sim W^{- 1}\left( \Psi_{dc0},\nu_{dc0} \right)\]
	, where \(\nu_{dc0} = p + 1 + 2\) and
	\(\Psi_{dc0} = \mathbf{I}_{p + 1} \times 10^{-4}\)
	
	\item
	Linear dynamics for latent vectors: \(\mathbf{A}\), \(\mathbf{b}\) and process noise \(\mathbf{Q}\).\\ 
	Denote \(\mathbf{F} = \begin{pmatrix}
		\mathbf{b}^{'} \\
		\mathbf{A}^{'} \\
	\end{pmatrix}\), \(\mathbf{f} = vec(\mathbf{F})\)
	
	\begin{align*}
		P\left( \mathbf{f},\ \mathbf{Q} \right) &= P(\mathbf{Q})P(\mathbf{f}|\mathbf{Q})\\
		\mathbf{Q} &\sim W^{- 1}(\Psi_{\mathbf{Q}_{0}},\nu_{\mathbf{Q}_{0}})\\
		\mathbf{f} &\sim N(\mathbf{f}_0, \mathbf{Q}\bigotimes \bm{\Lambda}_0^{-1})
	\end{align*}
	
	, where \(\mathbf{f}_0 = vec(\mathbf{F}_0)\).If the number of cluster is \(J\), \(\nu_{\mathbf{Q}_{0}} = Jp + 2\),
	\(\Psi_{\mathbf{Q}_{0}} = \mathbf{I}_{Jp} \times 10^{- 4}\). (To make the mean of \(\mathbf{Q}\) loosely centered around
	\(\mathbf{I}_{Jp} \times 10^{- 4}\)), \(\mathbf{F}_0 = \begin{pmatrix}
		\mathbf{0}'_{Jp} \\
		\mathbf{I}_{Jp} \\
	\end{pmatrix}\) and \(\bm{\Lambda}_0 = \mathbf{I}_{Jp + 1}\)
	
\end{enumerate}

\subsection{MCMC (Gibbs Sampler)}

\subsubsection{Update \(\left\{ \mathbf{x}_{t} \right\}_{t=1}^T\)}
Use Laplace approximation and make use of the block tri-diagonal Hessian.

Denote \(t^{\text{th}}\) column of mean firing rate and observation as \({\widetilde{\bm{\lambda}}}_{t} = \left( \lambda_{1t},\ldots,\lambda_{\text{Nt}} \right)'\)
and \({\widetilde{\mathbf{y}}}_{t} = (y_{1t},\ldots,y_{\text{Nt}})'\).
The linear mapping matrix for all observations is \(\mathbf{C}\), such that
\(\log{\widetilde{\bm{\lambda}}}_{t} = \mathbf{d} + \mathbf{C}\mathbf{x}_{t}\).
Let \(\mathbf{x} = \left( \mathbf{x'}_{1},\ldots,\mathbf{x'}_{T} \right)'\)and
\(f\left( \mathbf{x} \right) = \log{P(\mathbf{x}|\left\{ \mathbf{y}_{i} \right\}_{i = 1}^{N},\mathbf{C},\mathbf{Q}_{0},\mathbf{A},\mathbf{b},\mathbf{Q},\ldots)}\)
The first and second derivative with respect to \(\mathbf{x}\), for \(t=2, \ldots, T-1\):
\begin{align*}
	\frac{\partial f}{\partial\mathbf{x}_{1}} &= \mathbf{C'}\left( {\widetilde{\mathbf{y}}}_{1} - {\widetilde{\bm{\lambda}}}_{1} \right) - \mathbf{Q}_{0}^{- 1}\left( \mathbf{x}_{1} - \mathbf{x}_{0} \right) + \mathbf{A}'\mathbf{Q}^{- 1}\mathbf{(}\mathbf{x}_{2} - \mathbf{A}\mathbf{x}_{1} - \mathbf{b)} \\
	\frac{\partial f}{\partial\mathbf{x}_{t}} &= \mathbf{C'}\left( {\widetilde{\mathbf{y}}}_{t} - {\widetilde{\bm{\lambda}}}_{t} \right) - \mathbf{Q}^{- 1}\left( \mathbf{x}_{t} - \mathbf{A}\mathbf{x}_{t - 1} - \mathbf{b} \right) + \mathbf{A}'\mathbf{Q}^{- 1}(\mathbf{x}_{t + 1} - \mathbf{A}\mathbf{x}_{t} -\mathbf{b)}\\
	\frac{\partial f}{\partial\mathbf{x}_{T}} &= \mathbf{C'}\left( {\widetilde{\mathbf{y}}}_{T} - {\widetilde{\bm{\lambda}}}_{T} \right) - \mathbf{Q}^{- 1}\left( \mathbf{x}_{T} - \mathbf{A}\mathbf{x}_{T - 1} - \mathbf{b} \right)\\
	\frac{\partial^{2}f}{\partial\mathbf{x}_{1}\partial\mathbf{x}'_{1}} &= - \mathbf{C'}\text{Diag}\left( {\widetilde{\bm{\lambda}}}_{1} \right)\mathbf{C} - \mathbf{Q}_{0}^{- 1} - \mathbf{A}'\mathbf{Q}^{- 1}\mathbf{A}\\
	\frac{\partial^{2}f}{\partial\mathbf{x}_{t}\partial\mathbf{x}'_{t}} &= - \mathbf{C'}\text{Diag}\left( {\widetilde{\bm{\lambda}}}_{t} \right)\mathbf{C} - \mathbf{Q}^{- 1} - \mathbf{A}'\mathbf{Q}^{- 1}\mathbf{A}\\
	\frac{\partial^{2}f}{\partial\mathbf{x}_{T}\partial\mathbf{x}'_{T}} &= - \mathbf{C'}\text{Diag}\left( {\widetilde{\bm{\lambda}}}_{T} \right)\mathbf{C} - \mathbf{Q}^{- 1}\\
	\frac{\partial^{2}f}{\partial\mathbf{x}_{1}\partial\mathbf{x}'_{2}} &= \frac{\partial^{2}f}{\partial\mathbf{x}_{t}\partial\mathbf{x}'_{t + 1}} = \mathbf{A}'\mathbf{Q}^{- 1}
	&&\frac{\partial^{2}f}{\partial\mathbf{x}_{t}\partial\mathbf{x}'_{t - 1}} = \mathbf{Q}^{- 1}\mathbf{A}
\end{align*}

So, the gradient is:

\[\nabla = \frac{\partial f}{\partial\mathbf{x}} = \left( \left( \frac{\partial f}{\partial\mathbf{x}_{1}} \right)',\ \ldots,\left( \frac{\partial f}{\partial\mathbf{x}_{T}} \right)' \right)'\]

And the block tri-diagonal Hessian:

\[H = \frac{\partial^{2}f}{\partial\mathbf{x}\partial\mathbf{x}^{'}} = \begin{pmatrix}
	\frac{\partial^{2}f}{\partial\mathbf{x}_{1}\partial\mathbf{x}'_{1}} & \mathbf{A'}\mathbf{Q}^{- 1} & 0 & \cdots & 0 \\
	\mathbf{Q}^{- 1}\mathbf{A} & \frac{\partial^{2}f}{\partial\mathbf{x}_{2}\partial\mathbf{x}'_{2}} & \mathbf{A'}\mathbf{Q}^{- 1} & \cdots & \vdots \\
	0 & \mathbf{Q}^{- 1}\mathbf{A} & \frac{\partial^{2}f}{\partial\mathbf{x}_{3}\partial\mathbf{x}'_{3}} & \cdots & \vdots \\
	\vdots & \vdots & \vdots & \ddots & \vdots \\
	0 & \cdots & \cdots & \cdots & \frac{\partial^{2}f}{\partial\mathbf{x}_{T}\partial\mathbf{x}'_{T}} \\
\end{pmatrix}\]

Use Newton-Raphson to find
\(\bm{\mu}_{\mathbf{x}} =\text{argmax}_{\mathbf{x}}\left( f\left( \mathbf{x} \right) \right)\)
and \(\mathbf{\Sigma}_{\mathbf{x}}= -\left\lbrack \frac{\partial^{2}f}{\partial\mathbf{x}\partial\mathbf{x}'}\left. \ \mathbf{} \right|_{\mathbf{X} = \bm{\mu}_{\mathbf{X}}} \right\rbrack^{\mathbf{- 1}}\), such that \((P(\mathbf{x}|\left\{ \mathbf{y}_{i} \right\}_{i = 1}^{N},\ldots)\ \approx N\left(\bm{\mu}_{\mathbf{x}}, \mathbf{\Sigma}_{\mathbf{x}}\right)\).
When using Newton-Raphson (NR), \(H\backslash\nabla\) in MATLAB will make use
of block tri-diagonal structure automatically.

However, NR is not robust to bad initials. At the first few iterations, simply using fitting from previous step may lead to infinite Hessian. When the initial from previous step fails, use the approximation at recursive priors, , i.e. the adaptive smoother estimates, as the initial. The adaptive smoother estimates are from backward RTS smoother from adaptive filter, and the details about Poisson adaptive filter can be found in  \href{http://www.stat.columbia.edu/~liam/teaching/neurostat-spr11/papers/brown-et-al/eden2004.pdf}{Eden et al., 2004}.

To sample efficiently and make best use of sparse covariance, use Cholesky decomposition of
\(\mathbf{\Sigma}_{\mathbf{x}}^{- 1} = \mathbf{R}\mathbf{R}'\):
sample
\(\mathbf{Z} \sim N(\mathbf{R}'\mathbf{\mu}_{\mathbf{x}},\mathbf{I})\),
then
\(\mathbf{x} = \left( \mathbf{R}' \right)^{- 1}\mathbf{Z} \sim N(\bm{\mu}_{\mathbf{x}},\mathbf{\Sigma}_{\mathbf{x}})\).   

\textbf{Step Further:}\\
Using normal approximation may not be accurate enough, and this may lead to a bad mixing. We can step further to use the Metropolis-Hasting, based on the normal approximated variance. To be specific, the proposal distribution is \(Q(\mathbf{x}^{*}|\mathbf{x}^{(s)}) \sim N(\mathbf{x}^{(s)}, \alpha\mathbf{\Sigma}_{\mathbf{x}})\), where \(\alpha\) is a scalar to make the acceptance rate to be 0.4 to 0.6. According to \href{https://projecteuclid.org/journals/statistical-science/volume-16/issue-4/Optimal-scaling-for-various-Metropolis-Hastings-algorithms/10.1214/ss/1015346320.full}{Roberts and Rosenthal, 2001}. For high dimensional MH, the optimal proposal is \(N(x, 2.38^2\Sigma/d)\). See details \& experiments in Section \ref{sim}: Simulations.


\subsubsection{Update \(\mathbf{x}_{0}\)}
\[P\left( \mathbf{x}_{0}|\mathbf{x}_{1},\ \mathbf{Q}_{0}\ldots \right) \propto N(\mathbf{x}_{1}|\mathbf{x}_{0},\ \mathbf{Q}_{0})N(\mathbf{x}_{0}|\bm{\mu}_{\mathbf{x}_{00}},\ \mathbf{\Sigma}_{\mathbf{x}_{00}})\]
By conjugacy, \(\mathbf{x}_{0}|\mathbf{x}_{1},\ \mathbf{Q}_{0}\ldots \sim N(\bm{\mu}_{\mathbf{x}_{0}},\ \mathbf{\Sigma}_{\mathbf{x}_{0}})\)
\begin{align*}
	\mathbf{\Sigma}_{\mathbf{x}_{0}} &= \left\lbrack \mathbf{\Sigma}_{\mathbf{x}_{00}}^{- 1} + \mathbf{Q}_{0}^{- 1} \right\rbrack^{- 1}\\
	\bm{\mu}_{\mathbf{x}_{0}} &= \mathbf{\Sigma}_{\mathbf{x}_{0}}\left( \mathbf{\Sigma}_{\mathbf{x}_{00}}^{- 1}\bm{\mu}_{\mathbf{x}_{00}} + \mathbf{Q}_{0}^{- 1}\mathbf{x}_{1} \right)
\end{align*}

\subsubsection{Update \(\left\{ d_{i} \right\}_{i = 1}^{N}\) and
	\(\left\{ \mathbf{c}_{i} \right\}_{i = 1}^{N}\)}
To update efficiently, use Laplace approximation and the error is independent for each row (i.e. update things row by row). Denote
\(\left( d_{i},\mathbf{c}'_{i} \right)' = \bm{\zeta}_{i} \in \mathbb{R}^{p + 1}\)
and \(\left( 1,{\mathbf{x}_{t}'^{\left( z_{i} \right)}} \right) = {{\widetilde{\mathbf{x}}}_{t}'^{\left( z_{i} \right)}}\).
\begin{align*}
	P\left( \bm{\zeta}_{i} \middle| \mathbf{y}_{i},\ \left\{ \mathbf{x}_{t}^{\left( z_{i} \right)} \right\}_{t = 1}^{T},\ldots \right) &= \exp{f\left( \bm{\zeta}_{i} \right)} \approx N\left( \bm{\zeta}_{i}|\bm{\mu}_{\bm{\zeta}_{i}},\mathbf{\Sigma}_{\bm{\zeta}_{i}}\  \right)\\
	\frac{\partial f}{\partial\bm{\zeta}_{i}} &= \frac{\partial l}{\partial\bm{\zeta}_{i}} - {\mathbf{\Sigma}_{\text{dc}}^{\left( z_{i} \right)}}^{- 1}\left( \bm{\zeta}_{i} - \bm{\mu}_{\text{dc}}^{(z_{i})} \right) = \left\lbrack \sum_{t = 1}^{T}{\widetilde{\mathbf{x}}}_{t}^{\left( z_{i} \right)}\left( y_{\text{it}} - \lambda_{\text{it}} \right) \right\rbrack - {\mathbf{\Sigma}_{\text{dc}}^{\left( z_{i} \right)}}^{- 1}\left( \bm{\zeta}_{i} - \bm{\mu}_{\text{dc}}^{(z_{i})} \right)\\
	\frac{\partial^{2}f}{\partial\bm{\zeta}_{i}\partial\bm{\zeta}'_{i}} &= \frac{\partial^{2}l}{\partial\bm{\zeta}_{i}\partial\bm{\zeta}'_{i}} - {\mathbf{\Sigma}_{\text{dc}}^{\left( z_{i} \right)}}^{- 1} = -\left\lbrack \sum_{t = 1}^{T}{\lambda_{\text{it}}{\widetilde{\mathbf{x}}}_{t}^{\left( z_{i} \right)}{{\widetilde{\mathbf{x}}}_{t}'^{\left( z_{i} \right)}}} \right\rbrack - {\mathbf{\Sigma}_{\text{dc}}^{\left( z_{i} \right)}}^{- 1}
\end{align*}
, where \(l\) is Poisson log-likelihood. Then use Newton-Raphson to find
\(\bm{\mu}_{\bm{\zeta}_{i}} = \text{argmax}_{\bm{\zeta}_{i}}\left( f\left( \bm{\zeta}_{i} \right) \right)\)
and \(\mathbf{\Sigma}_{\bm{\zeta}_{i}} = -\left\lbrack \frac{\partial^{2}f}{\partial\bm{\zeta}_{i}\partial\bm{\zeta}'_{i}}\left.  \right|_{\bm{\zeta}_{i} = \bm{\mu}_{\bm{\zeta}_{i}}} \right\rbrack^{-1}\). If initial as the previous step fits fails, simply use prior mean of \(\bm{\mu}_{\bm{\zeta}_{i}}\), i.e. \(\bm{\delta}_{dc0}\).


%DETOUR: we can write things in the form of MV-(Poisson)-GLM as follows (in cluster j with \(n_j\) observations/ neurons, with corresponding loading be \(\mathbf{d}_{j}\) and \(\mathbf{C}_{j}\)):
%
%\[\log\begin{pmatrix}
%	\lambda_{11} & \cdots & \lambda_{1n_{j}} \\
%	\vdots & \ddots & \vdots \\
%	\lambda_{T1} & \cdots & \lambda_{Tn_{j}} \\
%\end{pmatrix} = \begin{pmatrix}
%	1 & \mathbf{x}_{1}^{'} \\
%	\vdots & \vdots \\
%	1 & \mathbf{x}_{T}^{'} \\
%\end{pmatrix}\begin{pmatrix}
%	\mathbf{d}_{j}^{'} \\
%	\mathbf{C}_{j}^{'} \\
%\end{pmatrix} + \mathbf{E}_j\]
%
%We can further organize things column-wise. let \(\begin{pmatrix}
%	1 & \mathbf{x}_{1}^{'} \\
%	\vdots & \vdots \\
%	1 & \mathbf{x}_{T}^{'} \\
%\end{pmatrix} = \mathbf{G}\), then the design matrix and corresponding log-mean response are \(\begin{pmatrix}
%\mathbf{G} & \mathbf{0} & \cdots & \mathbf{0} \\
%\mathbf{0} & \mathbf{G} & \cdots & \mathbf{0} \\
%\vdots & \vdots & \ddots & \vdots \\
%\mathbf{0} & \cdots & \cdots & \mathbf{G} \\
%\end{pmatrix}\) and \({\log\begin{pmatrix}
%	\lambda_{11}^{(j)} & \cdots & \lambda_{T1}^{(j)} & \lambda_{12}^{(j)} & \cdots & \lambda_{T2}^{(j)} & \cdots & \lambda_{Tn_{j}}^{(j)} \\
%\end{pmatrix}}'\).
%
%However, when I use the Laplace approximation and NR, because of the block-diagonal structure of design matrix, the error is always independent observation by observation. This is equivalent to observation-wise update as above.
%
%\textbf{Maybe I did something wrong?}
%The point to say this is to make loading within each cluster depends on each other. Check these two later(\href{https://www.tandfonline.com/doi/full/10.1080/03610926.2012.743565?journalCode=lsta20}{ref1} and \href{https://www.tandfonline.com/doi/full/10.1080/02664763.2021.1877637?src=recsys}{ref2}).
%
%Now I do it in another way: make the prior cluster-dependent...


\textbf{Step further}:
Again, use MH to sample the posterior, based on the normal approximated variance. Since the dimension of d and C is not large (just d = 3) in the simulation, I didn't use the optimal scalar yet. \textbf{Modify it later}.

\subsubsection{Update \(\left\{ \mathbf{\mu}_{\text{dc}}^{(j)} \right\}_{j}\) and \(\left\{ \mathbf{\Sigma}_{\text{dc}}^{(j)} \right\}_{j}\)} \label{loading prior}

Purpose: to make loading within each cluster depends on each other, and this will help with clustering. As above, denote \(\left( d_{i},\mathbf{c}'_{i} \right)' = \bm{\zeta}_{i} \in \mathbb{R}^{p + 1}\).
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	Mean \(\left\{ \bm{\mu}_{\text{dc}}^{(j)} \right\}_{j}\): by
	conjugacy, \(\bm{\mu}_{\text{dc}}^{(j)} \sim N\left( \bm{\delta}_{\text{dc}},\mathbf{T}_{\text{dc}}\  \right)\)
	\begin{align*}
		\mathbf{T}_{\text{dc}}^{- 1} &= \left( \mathbf{T}_{dc0}^{- 1} + n_{j}{\mathbf{\Sigma}_{\text{dc}}^{(j)}}^{- 1} \right)^{- 1}\\
		\bm{\delta}_{\text{dc}} &= \mathbf{T}_{\text{dc}}\left( \mathbf{T}_{dc0}^{- 1}\bm{\delta}_{dc0} + {\mathbf{\Sigma}_{\text{dc}}^{(j)}}^{- 1}\sum_{i:z_{i} = j}^{}\bm{\zeta}_{i} \right)
	\end{align*}
	\item
	Covariance \(\left\{ \mathbf{\Sigma}_{\text{dc}}^{(j)} \right\}_{j}\): by conjugacy, \(\mathbf{\Sigma}_{\text{dc}}^{(j)} \sim W^{- 1}\left( \Psi_{\text{dc}},\nu_{\text{dc}} \right)\)
	\begin{align*}
		\nu_{\text{dc}} &= n_{j} + \nu_{dc0}\\
		\Psi_{\text{dc}} &= \Psi_{dc0} + \sum_{i:z_{i} = j}^{}{\left( \bm{\zeta}_{i} - \bm{\mu}_{\text{dc}}^{(j)} \right)\left( \bm{\zeta}_{i} - \bm{\mu}_{\text{dc}}^{(j)} \right)'}
	\end{align*}
\end{enumerate}

\subsubsection{Update \(\mathbf{A}\), \(\mathbf{b}\) and \(\mathbf{Q}\)} 
Here, I only give the update for full \(\mathbf{Q}\). If we want to assume block-diagonal or diagonal structure of \(\mathbf{Q}\), just update things cluster-by-cluster or latent-by-latent.

As shown before, the problem is the usual Bayesian MV-GLM problem. Denote \(\mathbf{F} = \begin{pmatrix}
	\mathbf{b}^{'} \\
	\mathbf{A}^{'} \\
\end{pmatrix}\), \(\mathbf{f} = vec(\mathbf{F})\), \(\mathbf{Y}_\mathbf{bA} = (\mathbf{x}_{2}, \mathbf{x}_{3} \ldots, \mathbf{x}_{T})'\) and \(\mathbf{X}_\mathbf{bA} = \begin{pmatrix}
1 & \mathbf{x}_{1}^{'} \\
1 & \mathbf{x}_{2}^{'} \\
\vdots & \vdots \\
1 & \mathbf{x}_{T - 1}^{'} \\
\end{pmatrix}\)

\[\mathbf{Y}_\mathbf{bA} = \mathbf{X}_\mathbf{bA}\mathbf{F}+ \mathbf{E}\]

, where
\(\mathbf{E} = \left( \mathbf{\epsilon}_{1},\ldots,\mathbf{\epsilon}_{T - 1} \right)^{'}\) and \(\mathbf{\epsilon} \sim N(0,\mathbf{Q})\). Then posteriors are
\begin{align*}
	\mathbf{Q}|\mathbf{Y}_{\mathbf{bA}},\mathbf{X}_{\mathbf{bA}},\ldots &\sim W^{-1}(\Psi_{\mathbf{Q}}, \nu_{\mathbf{Q}})\\
	\mathbf{f}|\mathbf{Y}_{\mathbf{bA}},\mathbf{X}_{\mathbf{bA}},\mathbf{Q},\ldots &\sim N(vec(\mathbf{F}_\mathbf{bA}),  \mathbf{Q}\bigotimes \bm{\Lambda}_\mathbf{bA}^{-1})
\end{align*}
, with parameters be:
\begin{align*}
	\Psi_{\mathbf{Q}} &= \Psi_{\mathbf{Q}_{0}} + (\mathbf{Y}_\mathbf{bA} - \mathbf{X}_\mathbf{bA}\mathbf{F}_\mathbf{bA})'(\mathbf{Y}_\mathbf{bA} - \mathbf{X}_\mathbf{bA}\mathbf{F}_\mathbf{bA}) + (\mathbf{F}_\mathbf{bA} - \mathbf{F}_0)'\bm{\Lambda}_0(\mathbf{F}_\mathbf{bA} - \mathbf{F}_0)\\
	\nu_{\mathbf{Q}} &= \nu_{\mathbf{Q}_{0}} + T - 1\\
	\mathbf{F}_\mathbf{bA} &= (\mathbf{X}'_\mathbf{bA}\mathbf{X}_\mathbf{bA} + \bm{\Lambda}_0)^{-1}(\mathbf{X}'_\mathbf{bA}\mathbf{Y}_\mathbf{bA} + \bm{\Lambda}_0\mathbf{F}_0)\\
	\bm{\Lambda}_\mathbf{bA} &= \mathbf{X}'_\mathbf{bA}\mathbf{X}_\mathbf{bA} + \bm{\Lambda}_0
\end{align*}


%\section{Model Check [Problems for \(\mathbf{b}\) and \(\mathbf{A}\) partially solved]} \label{problems}
%
%\subsection{Estimation of \(\mathbf{d}\) and \(\mathbf{C}\) alone}
%First, turn off all others except for loading (related) parameters and set them to be true values. There are 2 versions: (1) update priors as in subsection \ref{loading prior} and (2) no update of priors, and just set the prior for each as the standard normal.
%For the estimation with prior update, the results are mean from iteration 50 to 100 (Figure \ref{loading alone}).
%\begin{figure}[h!]
%	\makebox[\linewidth][c]{%
%		\centering
%		\subfigure[]{\includegraphics[width=0.6\textwidth]{image016.png}}%
%		\subfigure[]{\includegraphics[width=0.6\textwidth]{image017.png}}%
%	}
%	\caption{estimation of loading}
%	\label{loading alone}
%\end{figure}
%
%The mixing of convergence for loading priors (in subsection \ref{loading prior}) are fine. The update of priors don't influence results a lot, but this will help clustering a lot. It seems estimation of loading is fine. Let's see what happens when estimation of latents \(\mathbf{x}_t\) is on at the same time.
%
%\subsection{Estimation of \(\mathbf{d}\), \(\mathbf{C}\) and \(\mathbf{x}_{t}\)}
%When the estimation of \(\mathbf{x}_t\) is on, we need more iterations. The results are mean from iteration 500 to 1000 (Figure \ref{loading and latent}).
%
%The estimation of loading:
%\begin{figure}[h!]
%	\makebox[\linewidth][c]{%
%		\centering
%		\subfigure[]{\includegraphics[width=0.6\textwidth]{image018.png}}%
%		\subfigure[]{\includegraphics[width=0.6\textwidth]{image020.png}}%
%	}
%	\caption{estimation of loading, with latents on}
%	\label{loading and latent}
%\end{figure}
%
%And the estimation of corresponding latents (Figure \ref{loading and latents}):
%\begin{figure}[h!]
%	\makebox[\linewidth][c]{%
%		\centering
%		\subfigure[no update of prior]{\includegraphics[width=0.6\textwidth]{image019.png}}%
%		\subfigure[with update of prior]{\includegraphics[width=0.6\textwidth]{image021.png}}%
%	}
%	\caption{estimation of loading}
%	\label{loading and latents}
%\end{figure}
%
%Things are still fine. The remaining parts may influence latent estimations are dynamics (\(\mathbf{b}\), \(\mathbf{A}\)) and prior process noise \(\mathbf{Q}\).
%
%\subsection{Estimation of \(\mathbf{b}\), \(\mathbf{A}\), \(\mathbf{Q}\) and \(\mathbf{x}_t\)}
%I show the results of block-diagonal (Figure \ref{bAQX_blkDiag}) and full (Figure \ref{bAQX_full}) \(\mathbf{Q}\). These results are from 2000 MCMC samples. The latent and dynamics are from average from 500 to 2000 iterations.
%
%\begin{figure}[h!]
%	\makebox[\linewidth][c]{%
%		\centering
%		\subfigure[trace]{\label{fig:a}\includegraphics[width=0.55\textwidth]{bAX_blkDiag_trace.png}}%
%		\subfigure[latent]{\label{fig:b}\includegraphics[width=0.55\textwidth]{bAX_blkDiag_latent.png}}%
%		\subfigure[dynamics \(\mathbf{A}\)]{\label{fig:c}\includegraphics[width=0.55\textwidth]{bAX_blkDiag_A.png}}%
%	}
%	\caption{sampling of $\mathbf{b}$, $\mathbf{A}$, $\mathbf{Q}$ and $\mathbf{x}_t$, block-diagonal process noise}
%	\label{bAQX_blkDiag}
%\end{figure}
%
%\begin{figure}[h!]
%	\makebox[\linewidth][c]{%
%		\centering
%		\subfigure[trace]{\label{fig:a}\includegraphics[width=0.55\textwidth]{bAX_full_trace.png}}%
%		\subfigure[latent]{\label{fig:b}\includegraphics[width=0.55\textwidth]{bAX_full_latent.png}}%
%		\subfigure[dynamics \(\mathbf{A}\)]{\label{fig:c}\includegraphics[width=0.55\textwidth]{bAX_full_A.png}}%
%	}
%	\caption{sampling of $\mathbf{b}$, $\mathbf{A}$, $\mathbf{Q}$ and $\mathbf{x}_t$, full process noise}
%	\label{bAQX_full}
%\end{figure}
%
%Both block-diagonal and full looks fine, but the mixing of  full process noise version looks better.


\section{Simulations}\label{sim}

\textbf{\textcolor{red}{All the simulation results are old and problematic. Update them later.}}

%There are two set of simulation examples. The first example is generated from LDS model directly, while in the second example the latents are generated directly without explicit specifying linear dynamics. In all following results, I fit things with both \href{https://github.com/weigcdsb/state-space-clustering/tree/main/LDS/blkDiag}{block-diagonal} and \href{https://github.com/weigcdsb/state-space-clustering/tree/main/LDS/full}{full} \(\mathbf{Q}\). The fitting results for block-diagonal version is a bit better, because the underlying true \(\mathbf{Q}\) is diagonal. In the following part, I only show results from block-diagonal fitting for labeled data. For unlabeled data, i.e. clustering, all the results can be found in this \href{https://github.com/weigcdsb/state-space-clustering/tree/main/results/gif}{folder}.
%
%\subsection{Simulation 1: Generate from LDS Directly}
%In this simulation, there are 3 clusters with 10 neurons in each cluster. The dimension of latents in each cluster is 2. In the linear dynamics, the bias term \(\mathbf{b}\) is zero. There's no within-population interaction but has some weak between-population interactions. In other words, the linear dynamics matrix \(\mathbf{A}\) is roughly diagonal. The details of simulation can be found in the first section of the \href{https://github.com/weigcdsb/state-space-clustering/blob/main/LDS/blkDiag/lds_sample_DP_blkDiag.m}{simulation 1}.
%
%\subsubsection{Labeled Data: No Clustering}
%The code for fitting is \href{https://github.com/weigcdsb/state-space-clustering/blob/main/LDS/blkDiag/lds_sample_DP_blkDiag.m}{here}. 
%
%Here, I first compare 10k iterations for three methods (consider high dimension of \(\mathbf{x}_t\)): (1) both \(\mathbf{x}_t\) and loading (\(\mathbf{d}\) and \(\mathbf{C}\)) are updated by normal approximation. (2) update \(\mathbf{x}_t\) by normal approximation, but update loading by MH. (3) update both \(\mathbf{x}_t\) and loading by MH.
%
%The fitting results are average of iteration 5k to 10k.
%
%\textbf{Update \(\mathbf{x}_t\) and (\(\mathbf{d}\), \(\mathbf{C}\)) by normal approximation}
%
%The trace plots are in Figure \ref{double N: trace}
%\begin{figure}[h!]
%	\makebox[\linewidth][c]{%
%		\centering
%		\subfigure[tracd of latent]{\label{fig:a}\includegraphics[width=0.55\textwidth]{xnorm_doubleN.png}}%
%		\subfigure[trace of loading and dynamics \(\mathbf{A}\)]{\label{fig:c}\includegraphics[width=0.55\textwidth]{trace_doubleN.png}}%
%	}
%	\caption{both Normal Approx., trace plots}
%	\label{double N: trace}
%\end{figure}
%
%Well, the chain hasn't achieve the stationary distribution. So there's no need to see further results. But to show the improvement and for comparison, I still show the fitting of overall firing rate, latents and dynamics in Figure \ref{double N: fit}.
%
%\begin{figure}[h!]
%	\makebox[\linewidth][c]{%
%		\centering
%		\subfigure[overall mean firing rate]{\label{fig:a}\includegraphics[width=0.55\textwidth]{FR_doubleN.png}}%
%		\subfigure[latents]{\label{fig:b}\includegraphics[width=0.55\textwidth]{latent_doubleN.png}}%
%		\subfigure[dynamics \(\mathbf{A}\)]{\label{fig:c}\includegraphics[width=0.55\textwidth]{A_doubleN.png}}%
%	}
%	\caption{both Normal Approx., fit}
%	\label{double N: fit}
%\end{figure}
%
%\textbf{Update \(\mathbf{x}_t\) by normal approximation and (\(\mathbf{d}\), \(\mathbf{C}\)) by MH}
%
%By using the normal approximated posterior variance as the proposal variance, the acceptance rates are around 0.45 (Since there are 30 neurons, there are 30 acceptance rates).
%The trace plots are in Figure \ref{dc MH: trace}
%\begin{figure}[h!]
%	\makebox[\linewidth][c]{%
%		\centering
%		\subfigure[tracd of latent]{\label{fig:a}\includegraphics[width=0.55\textwidth]{xnorm_dcMH.png}}%
%		\subfigure[trace of loading and dynamics \(\mathbf{A}\)]{\label{fig:c}\includegraphics[width=0.55\textwidth]{trace_dcMH.png}}%
%	}
%	\caption{latent Normal Approx. + loading MH, trace plots}
%	\label{dc MH: trace}
%\end{figure}
%
%Looks much better, although still not perfect yet. The fitting of overall firing rate, latents and dynamics are in Figure \ref{dc MH: fit}.
%
%\begin{figure}[h!]
%	\makebox[\linewidth][c]{%
%		\centering
%		\subfigure[overall mean firing rate]{\label{fig:a}\includegraphics[width=0.55\textwidth]{FR_dcMH.png}}%
%		\subfigure[latents]{\label{fig:b}\includegraphics[width=0.55\textwidth]{latent_dcMH.png}}%
%		\subfigure[dynamics \(\mathbf{A}\)]{\label{fig:c}\includegraphics[width=0.55\textwidth]{A_dcMH.png}}%
%	}
%	\caption{latent Normal Approx. + loading MH, fit}
%	\label{dc MH: fit}
%\end{figure}
%
%See, nearly perfect (especially) for latent.
%
%\textbf{Update both \(\mathbf{x}_t\) and (\(\mathbf{d}\), \(\mathbf{C}\)) by MH}
%
%The acceptance rate (by using the optimal scalar) for latent is 0.6227, which is a bit high (but acceptable). Again, the acceptance rates for loading are around 0.45.
%
%The trace plots are in Figure \ref{double MH: trace}
%\begin{figure}[h!]
%	\makebox[\linewidth][c]{%
%		\centering
%		\subfigure[tracd of latent]{\label{fig:a}\includegraphics[width=0.55\textwidth]{xnorm_doubleMH.png}}%
%		\subfigure[trace of loading and dynamics \(\mathbf{A}\)]{\label{fig:c}\includegraphics[width=0.55\textwidth]{trace_doubleMH.png}}%
%	}
%	\caption{both MH, trace plots}
%	\label{double MH: trace}
%\end{figure}
%
%The trace for dynamics is terrible... Maybe this is caused by high dimension of latent (\(d=6000\) in this case).
%
%The fitting of overall firing rate, latents and dynamics are in Figure \ref{double MH: fit}.
%
%\begin{figure}[h!]
%	\makebox[\linewidth][c]{%
%		\centering
%		\subfigure[overall mean firing rate]{\label{fig:a}\includegraphics[width=0.55\textwidth]{FR_doubleMH.png}}%
%		\subfigure[latents]{\label{fig:b}\includegraphics[width=0.55\textwidth]{latent_doubleMH.png}}%
%		\subfigure[dynamics \(\mathbf{A}\)]{\label{fig:c}\includegraphics[width=0.55\textwidth]{A_doubleMH.png}}%
%	}
%	\caption{both MH, fit}
%	\label{double MH: fit}
%\end{figure}
%
%OK, the fitting sucks...
%
%One natural idea is to combine MH and normal approximation in latent: at first few steps, use normal approximation at conditional posterior mode to bring things into somewhere close to mode quickly. Then use MH to stabilize everything.
%
%In the following, I still sample loading by MH. But for latent, the first half iteration is sampled by normal approximation, while the remainder is sampled from MH.
%
%The red line marks when I switch the sampling method for \(\mathbf{x}_t\).
% 
%The trace plots are in Figure \ref{half MH: trace}
%\begin{figure}[h!]
%	\makebox[\linewidth][c]{%
%		\centering
%		\subfigure[tracd of latent]{\label{fig:a}\includegraphics[width=0.55\textwidth]{xnorm_halfMH.png}}%
%		\subfigure[trace of loading and dynamics \(\mathbf{A}\)]{\label{fig:c}\includegraphics[width=0.55\textwidth]{trace_halfMH.png}}%
%	}
%	\caption{half MH for latent, trace plots}
%	\label{half MH: trace}
%\end{figure}
%
%The fitting of overall firing rate, latents and dynamics are in Figure \ref{half MH: fit}.
%
%\begin{figure}[h!]
%	\makebox[\linewidth][c]{%
%		\centering
%		\subfigure[overall mean firing rate]{\label{fig:a}\includegraphics[width=0.55\textwidth]{FR_halfMH.png}}%
%		\subfigure[latents]{\label{fig:b}\includegraphics[width=0.55\textwidth]{latent_halfMH.png}}%
%		\subfigure[dynamics \(\mathbf{A}\)]{\label{fig:c}\includegraphics[width=0.55\textwidth]{A_halfMH.png}}%
%	}
%	\caption{half MH for latent, fit}
%	\label{half MH: fit}
%\end{figure}
%
%Well, everything is stable now. But this implementation is too naive. Maybe use some more advanced adaptive sampling method?



%\begin{figure}[h!]
%	\centering
%	\includegraphics[width = .8\textwidth]{lds_blk_trace.png}
%	\caption{(Frobenius) norm for \(\mathbf{d}\), \(\mathbf{C}\), \(\mathbf{b}\) and \(\mathbf{A}\)}
%	\label{lds norms}
%\end{figure}\\
%Some results are in Figure \ref{fig:LDS labeled}. These are means from iteration 1000 to 2000.
%
%\begin{figure}[h!]
%	\makebox[\linewidth][c]{%
%		\centering
%		\subfigure[overall mean firing rate]{\label{fig:a}\includegraphics[width=0.55\textwidth]{lds_blk_FR.png}}%
%		\subfigure[latents]{\label{fig:b}\includegraphics[width=0.55\textwidth]{lds_blk_latent.png}}%
%		\subfigure[dynamics \(\mathbf{A}\)]{\label{fig:c}\includegraphics[width=0.55\textwidth]{lds_blk_A.png}}%
%	}
%	\caption{LDS sample with labels}
%	\label{fig:LDS labeled}
%\end{figure}
%
%Personally, I think the fitting is not bad and the mixing is fine (in previous version, we may need 10000 iterations to get a not bad result. But here 2000 iterations did even better than before).

%I also fit the model by assuming one cluster. Again, the fitting for overall mean firing rate is perfect. This is why it's important to make the loading, \(\mathbf{d}\) and \(\mathbf{C}\), also be cluster-dependent (loading within clusters are correlated).
%
%If the loading only depends on neuron index and will not change for different clustering assignments, it's impossible to do clustering (at least in this case), since the loading is enough to capture all the patterns.
%
%
%\subsubsection{Unlabeled Data: Clustering}
%To give the full path of clustering, I show results in GIFs. All results can be found in this \href{https://github.com/weigcdsb/state-space-clustering/tree/main/results/gif}{folder}. The code can be found in this \href{https://github.com/weigcdsb/state-space-clustering/tree/main/LDS/blkDiag}{folder}.
%
%Basically, in my current implementation, algorithms are good to merge clusters. However, generating new clusters is very hard... In other words, the newly generated cluster will usually not be sampled.
%
%\textbf{This is a big problem}. This makes DPMM loses its power. \textbf{Fix that later}.
%
%\subsection{Simulation 2: Generate Latents, without Specifying Linear Dynamics}
%In this simulation, there are again 3 clusters with 2 latents and 10 neurons in each. Now, the latents are generated directly without specifying the underlying linear dynamics of latents. However, each latent is generated independently, so the linear dynamics matrix \(\mathbf{A}\) should be roughly diagonal. The details of simulation can be found in the \href{https://github.com/weigcdsb/state-space-clustering/blob/main/LDS/blkDiag/unspecifiedA_sample_blkDiag.m}{code}.
%
%\subsubsection{Labeled Data: No Clustering}
%As in simulation 1, the data is fitted by the true cluster assignment (3 clusters) and forcing all neurons belong to single cluster. The code can be found \href{https://github.com/weigcdsb/state-space-clustering/blob/main/LDS/blkDiag/unspecifiedA_sample_blkDiag.m}{here}.
%
%
%\textbf{I have deleted the old results to avoid confusion. I will post the new results here later.}

%This time I ran 1000 iterations. The trace of the samples is shown in Figure \ref{noA norms}.
%\begin{figure}[h!]
%	\centering
%	\includegraphics[width = .8\textwidth]{noA_blk_trace.png}
%	\caption{(Frobenius) norm for \(\mathbf{d}\), \(\mathbf{C}\), \(\mathbf{b}\) and \(\mathbf{A}\)}
%	\label{noA norms}
%\end{figure}\\
%
%Some results are in Figure \ref{fig:noA labeled}. These are means from iteration 500 to 1000.
%
%\begin{figure}[h!]
%	\makebox[\linewidth][c]{%
%		\centering
%		\subfigure[overall mean firing rate]{\label{fig:a}\includegraphics[width=0.55\textwidth]{noA_blk_FR.png}}%
%		\subfigure[latents]{\label{fig:b}\includegraphics[width=0.55\textwidth]{noA_blk_latent.png}}%
%		\subfigure[dynamics \(\mathbf{A}\)]{\label{fig:c}\includegraphics[width=0.55\textwidth]{noA_blk_A.png}}%
%	}
%	\caption{LDS sample with labels}
%	\label{fig:noA labeled}
%\end{figure}
%
%This time the mixing of loading (\(\mathbf{d}\) and \(\mathbf{C}\)) is not fine. That reminds me that \textbf{I may need to make loading within cluster related by MV-(Poisson)-GLM directly.}


%\subsubsection{Unlabeled Data: Clustering}
%\textbf{The clustering results are old. Update when the MFM is implemented}
%
%Again, all results can be found in this \href{https://github.com/weigcdsb/state-space-clustering/tree/main/results/gif}{folder}. The code can be found in this \href{https://github.com/weigcdsb/state-space-clustering/tree/main/LDS/blkDiag}{folder}.
%
%Besides fitting 3 clusters with 10 neurons each, I further fit a larger scale example (50 neurons for each cluster). 

\section{Problem from Affine Transformation}
Let me review the model first.\\
Assume there are \(J\)
clusters, with \(n\) neurons in each cluster. Each cluster is governed
by \(p\) latent vectors. \(\bm{\lambda}_{t}^{(j)} \in \mathbb{R}^{n}\),
\(\mathbf{x}_{t}^{(j)} \in \mathbb{R}^{p}\), \(\mathbf{d}^{(j)} \in \mathbb{R}^{n}\) and
\(\mathbf{C}^{(j)} \in \mathbb{R}^{n \times p}\), with \(j = 1,\ldots,J\), be the
mean firing rate, latent state, intercept and loading (at time $t$) for
each cluster. Then I just stack all the thing together, and denote
corresponding things as \(\bm{\lambda}_{t} \in R^{\text{Jn}}\),
\(\mathbf{x}_{t} \in \mathbb{R}^{Jp}\), \(\mathbf{d} \in \mathbb{R}^{\text{Jn}}\)
and \(\mathbf{C} \in \mathbb{R}^{Jn \times Jp}\). Notice \(\mathbf{C}\) is block
diagonal.

\[\log{\begin{pmatrix}
		\ \bm{\lambda}_{t}^{(1)} \\
		\vdots \\
		\ \bm{\lambda}_{t}^{(J)} \\
	\end{pmatrix} =}\log\bm{\lambda}_{t} = \begin{pmatrix}
	\ \mathbf{d}^{(1)} \\
	\vdots \\
	\ \mathbf{d}^{(J)} \\
\end{pmatrix} + \begin{pmatrix}
	\mathbf{C}^{(1)} & \cdots & 0 \\
	\vdots & \ddots & \vdots \\
	0 & \cdots & \mathbf{C}^{(J)} \\
\end{pmatrix}\begin{pmatrix}
	\ \mathbf{x}_{t}^{(1)} \\
	\vdots \\
	\ \mathbf{x}_{t}^{(J)} \\
\end{pmatrix} = \mathbf{d} + \mathbf{C}\mathbf{x}_{t}\]

The latent states progress linearly with a Gaussian noise:

\[\mathbf{x}_{t + 1}|\mathbf{x}_{t} \sim N(\mathbf{A}\mathbf{x}_{t} + \mathbf{b},\ \mathbf{Q})\]

, where \(\mathbf{A} \in \mathbb{R}^{Jp \times Jp}\),
\(\mathbf{b} \in \mathbb{R}^{Jp}\) and
\(\mathbf{Q} \in \mathbb{R}^{Jp \times Jp}\).

Now assume we take an affine transformation of latent states as
\(\mathbf{x}_{t}^{*} = \mathbf{M}\mathbf{x}_{t} + \mathbf{g}\). To
preserve the block diagonal structure of \(\mathbf{C}\), \(\mathbf{M}\)
is also block diagonal. Then,
\begin{align*}
	\log\bm{\lambda}_{t} &= \mathbf{d} + \mathbf{C}\mathbf{M}^{- 1}\left( \mathbf{M}\mathbf{x}_{t} + \mathbf{g} \right) - \mathbf{C}\mathbf{M}^{- 1}\mathbf{g}\\
	&= \left( \mathbf{d} - \mathbf{C}\mathbf{M}^{- 1}\mathbf{g} \right) + \mathbf{C}\mathbf{M}^{- 1}(\mathbf{M}\mathbf{x}_{t} + \mathbf{g})\\
	\mathbf{M}\mathbf{x}_{t + 1} + \mathbf{g}|\mathbf{x}_{t} &\sim N_{Jp}\left( \mathbf{M}\left( \mathbf{A}\mathbf{x}_{t} + \mathbf{b} \right)+\mathbf{g}, \mathbf{MQM}' \right)\\
	&= N_{Jp}(\mathbf{MAM}^{- 1}\left( \mathbf{M}\mathbf{x}_{t} + \mathbf{g} \right) + \left( \mathbf{Mb} + \mathbf{g} - \mathbf{MAM}^{- 1}\mathbf{g} \right), \mathbf{MQM}')
\end{align*}

That means, when \(\left\{ \mathbf{d},\mathbf{C},\mathbf{x}_t,\mathbf{A},\mathbf{b},\mathbf{Q} \right\}\) is a solution,
\(\left\{ \mathbf{d}^*,\mathbf{C}^*,\mathbf{x}^*_t,\mathbf{A}^*,\mathbf{b}^*,\mathbf{Q}^* \right\}\) is also a
solution, where
\(\mathbf{d}^* = \mathbf{d} - \mathbf{C}\mathbf{M}^{- 1}\mathbf{g}\),
\(\mathbf{C}^* = \mathbf{CM}^{- 1}\), \(\mathbf{x}_{t}^{*} = \mathbf{M}\mathbf{x}_{t} + \mathbf{g}\),
\(\mathbf{A}^* = \mathbf{MAM}^{- 1}\),
\(\mathbf{b}^* = \mathbf{Mb} + \mathbf{g} - \mathbf{MAM}^{- 1}\mathbf{g}\)
and \(\mathbf{Q}^* = \mathbf{MQM}'\), for any block diagonal \(\mathbf{M}\). Therefore, we must add some constraints to ensure the unique solution/ stationary
posterior!

\subsection{Constraints}
An affine transformation contains translation and linear transformation. 

\subsubsection{Translation}
There are two strategies: (1) just ignore \(\mathbf{d}\) and let \(\mathbf{Cx}_{t}\)
absorb all the intercepts; (2) center \(\mathbf{x}_t\) such that mean for each
row of \((\mathbf{x}_{1},\ldots,\mathbf{x}_{T})\) is 0. However, because of the block-diagonal constraints on $\mathbf{M}$, the strategy (1) may be problematic. Moreover, when $\mathbf{d}$ is non-zero, the \(\mathbf{x}_t\) will looks ugly. Another benefit for (2) is that we can use $\mathbf{d}$ to help clustering. However, the constraint on translation cannot resolve problems from linear transformation, since any linear transformation will preserve the zero row mean in \((\mathbf{x}_{1},\ldots,\mathbf{x}_{T})\).

\subsubsection{Linear Transformation}
To mimic PCA, one natural way is to make the loading orthogonal. Again, I'm considering two strategies: (1) make
\(\mathbf{C}'^{(j)}\mathbf{C}^{(j)} = \mathbf{I}_{p}\); (2) make \( (\mathbf{C}'^{(1)},\cdots, \mathbf{C}'^{(J)})
\begin{pmatrix}
	\mathbf{C}^{(1)} \\
	\vdots \\
	\mathbf{C}^{(J)} \\
\end{pmatrix} = \mathbf{I}_{p}\). Personally, I prefer the first one, for both computational and clustering reasons. Computationally, if I do things cluster-wise, I can easily use parallel when dimension becomes large.

\textbf{However, this is not enough!} Consider \(\mathbf{x}_{t}^{*} = \mathbf{Ux}_{t}\), to
preserve the orthogonality and block diagonal on \(\mathbf{C}\), \(\mathbf{U}\) must be
block diagonal and each diagonal block \(\mathbf{U}^{(j)}\) is an orthogonal matrix (need to be the same for constraint (2)). More explicitly,

\[\begin{pmatrix}
	\mathbf{C}^{(1)} & \cdots & \mathbf{0} \\
	\vdots & \ddots & \vdots \\
	\mathbf{0} & \cdots & \mathbf{C}^{(J)} \\
\end{pmatrix}\begin{pmatrix}
	\mathbf{U}^{'(1)} & \cdots & \mathbf{0} \\
	\vdots & \ddots & \vdots \\
	\mathbf{0} & \cdots & \mathbf{U}^{'(J)} \\
\end{pmatrix} = \begin{pmatrix}
	\mathbf{C}^{(1)}\mathbf{U}^{'(J)} & \cdots & \mathbf{0} \\
	\vdots & \ddots & \vdots \\
	\mathbf{0} & \cdots & \mathbf{C}^{(1)}\mathbf{U}^{'(J)} \\
\end{pmatrix}\]

This transformation will also preserve the Frobenius norm of \(\mathbf{x}_{t}\),
but the element value is not preserved. To ensure the unique solution,
we need to add more constraints on linear dynamics, i.e. \(\mathbf{A}\) or/and
\(\mathbf{Q}\). Simply making \(\mathbf{A}\) or \(\mathbf{Q}\) be block diagonal or even diagonal
with same absolute value (e.g., \(\mathbf{I}_{Jp}\)) is still not enough. One solution is to force at least one of \(\mathbf{A}\) and \(\mathbf{Q}\) be diagonal.

Take \(\mathbf{A}\) for example. Assume the true \(\mathbf{A}\) is diagonal and true
\(\mathbf{C}^{\{ j\}}\) has orthogonal column. As shown previous, the linear transformation \(\mathbf{U}\) must be block diagonal and each diagonal block
\(\mathbf{U}^{(j)}\) is an orthogonal matrix. Further, to preserve the diagonal
structure of \(\mathbf{A}\) (the elements on the diagonal are generally not the
same), we need \(\mathbf{U}^{(j)}\) to be diagonal. These two constraints force
\(\mathbf{U}\) be diagonal with diagonal elements be \(\pm 1\). So the solution
is unique up to sign. The same rationale for constraint on \(\mathbf{Q}\).

\textbf{The above argument reminds me that I may need one more tiny constraint to solve the sign flipping issue. Do it later.}

In summary, we need add three constraints: (1) translation of \(\mathbf{x}_{t}\),
(2) orthogonality of loading and (3) diagonality of linear dynamics. In the following implementation, I tried 3 sets of constraints:

\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	row mean in \((\mathbf{x}_{1},\ldots,\mathbf{x}_{T})\) is 0,
	\(\mathbf{C}'^{(j)}\mathbf{C}^{(j)} = \mathbf{I}_{p}\), \(\mathbf{A}\) is full while \(\mathbf{Q}\) is
	diagonal
	\item
	row mean in \((\mathbf{x}_{1},\ldots,\mathbf{x}_{T})\) is 0,
	\(\mathbf{C}'^{(j)}\mathbf{C}^{(j)} = \mathbf{I}_{p}\), \(\mathbf{Q}\) is full while \(\mathbf{A}\) is
	diagonal
	\item
	row mean in \((\mathbf{x}_{1},\ldots,\mathbf{x}_{T})\) is 0,
	\(\mathbf{C}'^{(j)}\mathbf{C}^{(j)} = \mathbf{I}_{p}\), both \(\mathbf{A}\) and \(\mathbf{Q}\) are diagonal
\end{enumerate}

Constraint (1) and (2) allows interaction between clusters.

Although, constraint (1) (diagonal $\mathbf{Q}$) is valid in theory, it doesn't work well in practice. Since elements $\mathbf{Q}$ is usually very small (~$10^{-3} ot 10^{-5}$).
Therefore, I will discard this constraint later.

\subsection{Constraint Implementation}

\subsubsection{Translation}
The constraint on translation is easy, just row-center the sample of \((\mathbf{x}_{1},\ldots,\mathbf{x}_{T})\) in each iteration. We can get the transformed full conditional distribution after some algebra. Let \(\mathbf{X} = \left( \mathbf{x}_{1},\ldots,\mathbf{x}_{T} \right) = \begin{pmatrix}
	\mathbf{X}_{(1)}^{'} \\
	\vdots \\
	\mathbf{X}_{(Jp)}^{'} \\
\end{pmatrix}\), where \(\mathbf{x}_{(k)} \in \mathbb{R}^{T}\) is the k-th row vector.
Then the k-th row mean is
\({\overline{\mathbf{x}}}_{(k)} = \frac{1}{T}\mathbf{x}_{(k)}^{'}\mathbf{1}_{T}\). The unconstraint full conditional distribution of \(vec(\mathbf{X})\) is derived in previous section. After
normalization in each iteration, we are actually sampling from
\(P(vec(\mathbf{X}) - \mathbf{1}_{T}\bigotimes\begin{pmatrix}
	{\overline{\mathbf{x}}}_{(1)} \\
	\vdots \\
	{\overline{\mathbf{x}}}_{(Jp)} \\
\end{pmatrix}|\mathbf{Y},\ldots)\). Notice that,
\[\mathbf{1}_{T}\bigotimes\begin{pmatrix}
	{\overline{\mathbf{x}}}_{(1)} \\
	\vdots \\
	{\overline{\mathbf{x}}}_{(Jp)} \\
\end{pmatrix} = \mathbf{1}_{T}\bigotimes\frac{1}{T}\mathbf{X1}_{T} = \frac{1}{T}vec\left( \mathbf{1X}_{T}\mathbf{1}_{T}^{'} \right) = \frac{\mathbf{J}_{T}\bigotimes \mathbf{I}_{Jp}}{T}vec(\mathbf{X})\]
, then we are just sampling from full conditional distribution of
\((\mathbf{I}_{JpT} - \frac{\mathbf{J}_{T}\bigotimes \mathbf{I}_{Jp}}{T})vec(\mathbf{X})\)

\subsubsection{Linear Transformation}
The constraint on orthogonality is kind of tricky. Here, I just transformed \(\mathbf{C}^{(j)}\) be
\(\mathbf{C}^{(j)} = \mathbf{K}^{(j)}\left( \mathbf{K}^{'(j)}\mathbf{K}^{(j)} \right)^{- 1/2}\). Therefore,
we can just sample \(\mathbf{K}^{(j)}\) without any constrained. Since
\(\mathbf{K}^{(j)}\) has the same structure as \(\mathbf{C}^{(j)}\), we can put some
hierarchical prior to model \(\mathbf{C}^{(j)}\). Specially, when
\(\mathbf{K}^{(j)} \sim MN_{n,p}(\mathbf{0},\bm{\Sigma}^{(j)},\mathbf{I})\) (\(MN_{n,p}\) means a matrix
normal distribution), \(\mathbf{C}^{(j)} \sim MACG(\bm{\Sigma}^{(j)})\).
\(MACG(\bm{\Sigma})\) means the matrix angular central Gaussian distribution,
with density
\(f_{\mathbf{X}}(\mathbf{X}) = |\bm{\Sigma}\left. \  \right|^{- p/2}{|\mathbf{X}'\Sigma^{- 1}\mathbf{X}|}^{- n/2}\).
When \(\bm{\Sigma}^{(j)} = \mathbf{I}_{n}\) or \(n = p\), \(\mathbf{C}^{(j)}\) is just
uniformly distributed on the Stiefel manifold
\(S(p,n) = \left\{ \mathbf{C} \in \mathbb{R}^{n \times p} \right|\mathbf{C}'\mathbf{C} = \mathbf{I}_{p}\}\).

\textbf{This reminds me to fix the problem when \(n < p\)}, which may happen when doing clustering. One easy solution is just use the constraint
\( (\mathbf{C}'^{(1)},\cdots, \mathbf{C}'^{(J)})
\begin{pmatrix}
	\mathbf{C}^{(1)} \\
	\vdots \\
	\mathbf{C}^{(J)} \\
\end{pmatrix} = \mathbf{I}_{p}\), or just merge clusters with \(n < p\) to other
clusters. Fix that.

Since now there's no closed form for full conditional, I just sample it by no U-turn sampler (NUTS).

The diagonality in linear dynamics is trivial, except for the constraint with diagonal \(\mathbf{A}\) and full \(\mathbf{Q}\). This needs some algebra. Denote the diagonal \(\mathbf{A}\) as \(\mathbf{A} = diag(a_{1},\ldots,a_{Jp})\). Then,

\[\mathbf{Y}_{BA} = \begin{pmatrix}
	\mathbf{x}_{2}^{'} \\
	\mathbf{x}_{3}^{'} \\
	\vdots \\
	\mathbf{x}_{T}^{'} \\
\end{pmatrix} = \begin{pmatrix}
	1 & x_{1,1} & \cdots & x_{1,Jp} \\
	1 & x_{2,1} & \cdots & x_{2,Jp} \\
	\vdots & \vdots & \vdots & \vdots \\
	1 & x_{T - 1,1} & \cdots & x_{T - 1,Jp} \\
\end{pmatrix}\begin{pmatrix}
	b_{1} & \cdots & b_{Jp} \\
	a_{1} & \cdots & 0 \\
	\vdots & \ddots & \vdots \\
	0 & \cdots & a_{Jp} \\
\end{pmatrix} + \mathbf{E} = \mathbf{X}_{BA}\mathbf{F}_{BA} + \mathbf{E}\]

, where
\(\mathbf{E} = \left( \bm{\epsilon}_{1},\ldots,\bm{\epsilon}_{T - 1} \right)^{'}\)
and \(\bm{\epsilon}_{t} \sim N(\mathbf{0},\mathbf{Q})\). Notice that
\(\mathbf{x}_{t} = (x_{t, 1}, \cdots x_{t, Jp})'\).

Equivalently, we can rewrite the model as:

\[\mathbf{Y}_{ba} = vec\left(\mathbf{Y}_{BA} \right) = \begin{pmatrix}
	1 & x_{1,1} & \cdots & 0 & 0 \\
	\vdots & \vdots & \vdots & \vdots & \vdots \\
	1 & x_{T - 1,1} & \cdots & 0 & 0 \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	0 & 0 & \cdots & 1 & x_{1,Jp} \\
	\vdots & \vdots & \vdots & \vdots & \vdots \\
	0 & 0 & \cdots & 1 & x_{T - 1,Jp} \\
\end{pmatrix}\begin{pmatrix}
	b_{1} \\
	a_{1} \\
	\vdots \\
	b_{Jp} \\
	a_{Jp} \\
\end{pmatrix} + vec(\mathbf{E}) = \mathbf{X}_{ba}\mathbf{f}_{ba} + vec(\mathbf{E})\]

, where
\(Cov\left(vec(\mathbf{E}) \right) = \mathbf{Q}\bigotimes \mathbf{I}_{T - 1}\).

If we use the prior:
\begin{align*}
	\mathbf{Q} &\sim W^{- 1}(\Psi_{Q_{0}},\nu_{Q_{0}})\\
	\mathbf{f}_{ba}|\mathbf{Q} &\sim N_{2Jp}(\mathbf{1}_{Jp}\bigotimes \mathbf{f}_{ba,0},\ \mathbf{Q}\bigotimes\bm{\Lambda}_{ba,0}^{- 1})
\end{align*}

, where \(\mathbf{f}_{ba,0} = (0, 1)'\) and \(\bm{\Lambda}_{ba,0} = \mathbf{I}_{2}\). Then the
corresponding prior for \(vec(\mathbf{F}_{BA})\) is:

\[vec\left( \mathbf{F}_{BA} \right)|\mathbf{Q} \sim N(vec\left( \mathbf{F}_{BA,0} \right),\ \mathbf{Q}\bigotimes\bm{\Lambda}_{BA,0}^{- 1})\]

, where \(\mathbf{F}_{BA,0} = \begin{pmatrix}
	\mathbf{0}_{Jp}^{'} \\
	\mathbf{I}_{Jp} \\
\end{pmatrix}\) and \(\bm{\Lambda}_{BA,0} = \mathbf{I}_{Jp + 1}\).

Then the posteriors are:
\begin{align*}
	\mathbf{Q}|\mathbf{Y},\ldots &\sim W^{- 1}(\Psi_{Q},\nu_{Q})\\
	\mathbf{f}_{ba}|\mathbf{Y},\mathbf{Q},\ldots &\sim N({\widehat{\mathbf{f}}}_{ba},\ (\mathbf{Q}\bigotimes \mathbf{I}_{2})\Lambda_{ba}^{- 1})
\end{align*}

, with parameters be:
\begin{align*}
	\bm{\Lambda}_{ba} &= \mathbf{X}_{ba}^{'}\mathbf{X}_{ba} + \mathbf{I}_{Jp}\bigotimes\bm{\Lambda}_{ba,0}\\
	{\widehat{\mathbf{f}}}_{ba} &= \bm{\Lambda}_{ba}^{- 1}\left( \mathbf{X}_{ba}^{'}\mathbf{Y}_{ba} + \ \mathbf{1}_{Jp}\bigotimes\bm{\Lambda}_{ba,0}\mathbf{f}_{ba,0} \right)
\end{align*}

The corresponding estimate for \(\mathbf{F}_{BA}\) is denoted as
\({\widehat{\mathbf{F}}}_{BA}\), which is just a transformation of \({\widehat{\mathbf{f}}}_{ba}\).
\begin{align*}
	\Psi_{Q} &= \Psi_{Q0} + \left( \mathbf{Y}_{BA} - \mathbf{X}_{BA}{\widehat{\mathbf{F}}}_{BA} \right)^{'}\left( \mathbf{Y}_{BA} - \mathbf{X}_{BA}{\widehat{\mathbf{F}}}_{BA} \right) + \left( {\widehat{\mathbf{F}}}_{BA} - \mathbf{F}_{BA,0} \right)^{'}\Lambda_{BA,0}^{- 1}\left( {\widehat{\mathbf{F}}}_{BA} - \mathbf{F}_{BA,0} \right)\\
	\nu_{Q} &= \nu_{Q0} + T - 1
\end{align*}

\subsection{Simulation}
The simulation example is as before, and all the code can be found in \href{https://github.com/weigcdsb/state-space-clustering/tree/main/LDS/blkDiag}{this folder}.

Since now we constrain the loading has orthogonal column for each cluster, just showing the Frobenius norm of $\mathbf{x}_t$ is not enough to check convergence. Here, I further check the value of $\mathbf{x}_{[T/2]}$.

For \(\mathbf{C}^{(j)} = \mathbf{K}^{(j)}\left( \mathbf{K}^{'(j)}\mathbf{K}^{(j)} \right)^{- 1/2}\), I just put a simple hierarchical prior on $\mathbf{K}^{(j)}$ such that each element of $\mathbf{K}^{(j)}$ is i.i.d. $N(\mu_{k}^{(j)}, (\sigma^{(j)}_{k})^2)$. Well, maybe we can put some more careful priors later to reflect the covariance among neurons.

Basically, I ran both MCMCs for 10,000 iterations. Here, I show 6 sets of plots:
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	Full trace of Frobenius norm in $\mathbf{X} = (\mathbf{x}_{1},\ldots,\mathbf{x}_{T})$ and the value of $\mathbf{x}_{T/2} = \mathbf{x}_{500}$
	\item
	Full trace of 2-norm/ Frobenius norm in $\mathbf{d}$, $\mathbf{b}$, $\mathbf{A}$ and $\mathbf{Q}$ 
	\item
	partial trace plot of (1) from iteration 1 to 100
	\item
	partial trace plot of (2) from iteration 1 to 100
	\item
	average mean firing rate from iteration 5000 to 10,000.
	\item
	average $\mathbf{X}$ from iteration 5000 to 10,000.
\end{enumerate}

\subsubsection{\(\mathbf{Q}\) is full while \(\mathbf{A}\) is diagonal}
\begin{figure}[h!]
	\makebox[\linewidth][c]{%
		\centering
		\subfigure[tracd of $\mathbf{X}$]{\label{fig:a}\includegraphics[width=0.75\textwidth]{trace_X_full_diagA.png}}%
		\subfigure[trace of loading and dynamics]{\label{fig:b}\includegraphics[width=0.75\textwidth]{trace_dbAQ_full_diagA.png}}%
	}
	\caption{\(\mathbf{Q}\) is full while \(\mathbf{A}\) is diagonal, full trace}
	\label{full trace, diag A}
\end{figure}

\begin{figure}[h!]
	\makebox[\linewidth][c]{%
		\centering
		\subfigure[tracd of $\mathbf{X}$]{\label{fig:a}\includegraphics[width=0.75\textwidth]{trace_X_part_diagA.png}}%
		\subfigure[trace of loading and dynamics]{\label{fig:b}\includegraphics[width=0.75\textwidth]{trace_dbAQ_part_diagA.png}}%
	}
	\caption{\(\mathbf{Q}\) is full while \(\mathbf{A}\) is diagonal, partial trace}
	\label{partial trace, diag A}
\end{figure}

\begin{figure}[h!]
	\makebox[\linewidth][c]{%
		\centering
		\subfigure[mean firing rate]{\label{fig:a}\includegraphics[width=0.75\textwidth]{FR_diagA.png}}%
		\subfigure[latent vectors]{\label{fig:b}\includegraphics[width=0.75\textwidth]{latent_diagA.png}}%
	}
	\caption{\(\mathbf{Q}\) is full while \(\mathbf{A}\) is diagonal, mean FR and $\mathbf{x}_t$}
	\label{FR and X, diag A}
\end{figure}
\clearpage

\subsubsection{both \(\mathbf{A}\) and \(\mathbf{Q}\) are diagonal}
\begin{figure}[h!]
	\makebox[\linewidth][c]{%
		\centering
		\subfigure[tracd of $\mathbf{X}$]{\label{fig:a}\includegraphics[width=0.75\textwidth]{trace_X_full_diagAQ.png}}%
		\subfigure[trace of loading and dynamics]{\label{fig:b}\includegraphics[width=0.75\textwidth]{trace_dbAQ_full_diagAQ.png}}%
	}
	\caption{both \(\mathbf{A}\) and \(\mathbf{Q}\) are diagonal, full trace}
	\label{full trace, diag AQ}
\end{figure}

\begin{figure}[h!]
	\makebox[\linewidth][c]{%
		\centering
		\subfigure[tracd of $\mathbf{X}$]{\label{fig:a}\includegraphics[width=0.75\textwidth]{trace_X_part_diagAQ.png}}%
		\subfigure[trace of loading and dynamics]{\label{fig:b}\includegraphics[width=0.75\textwidth]{trace_dbAQ_part_diagAQ.png}}%
	}
	\caption{both \(\mathbf{A}\) and \(\mathbf{Q}\) are diagonal, partial trace}
	\label{partial trace, diag AQ}
\end{figure}

\begin{figure}[h!]
	\makebox[\linewidth][c]{%
		\centering
		\subfigure[mean firing rate]{\label{fig:a}\includegraphics[width=0.75\textwidth]{FR_diagAQ.png}}%
		\subfigure[latent vectors]{\label{fig:b}\includegraphics[width=0.75\textwidth]{latent_diagAQ.png}}%
	}
	\caption{both \(\mathbf{A}\) and \(\mathbf{Q}\) are diagonal, mean FR and $\mathbf{x}_t$}
	\label{FR and X, diag AQ}
\end{figure}

We can see that either is fine, and the convergence is achieved at nearly 20 to 30 iterations.

\textbf{In summary, to ensure convergence, we can put the following two constraints:}
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	row mean in \((\mathbf{x}_{1},\ldots,\mathbf{x}_{T})\) is 0,
	\(\mathbf{C}'^{(j)}\mathbf{C}^{(j)} = \mathbf{I}_{p}\), \(\mathbf{Q}\) is full while \(\mathbf{A}\) is
	diagonal
	\item
	row mean in \((\mathbf{x}_{1},\ldots,\mathbf{x}_{T})\) is 0,
	\(\mathbf{C}'^{(j)}\mathbf{C}^{(j)} = \mathbf{I}_{p}\), both \(\mathbf{A}\) and \(\mathbf{Q}\) are diagonal
\end{enumerate}

If we really care about the interactions between clusters, just use constraint (1). Otherwise, either is fine.

\subsection{Comments on Previous Models}
When fitting the LDS or factor analysis (e.g. GPFA) model, I may only believe the temporal pattern or the change of the pattern. The pattern itself at a specific time point doesn't make a lot of sense, because of the affine transformation invariance.

In previous work, they fit models by EM, variational Bayes or some other MAP/ MLE method. \textbf{This is quite dangerous} if they don't check the uniqueness (Thanks to the gold standard MCMC, otherwise I will never check this issue...). 

In the PLDS code that Ian shared me before, the default model has no constraint. They fit it by EM, and use ELBO as the convergence criteria. Although the parameters are not unique, the likelihood keeps the same, using ELBO is justified. However, since there's no randomness in EM, it's impossible to detect the non-uniqueness. Actually, I think their results are even not local optima, but just a single point in the optimal surface (I guess if they use something like stochastic EM, they will find the issue).  

The PLDS also provides some constraint options, but some of them are still not enough to ensure unique solution. BTW, this might be another reason they choose to use EM, since adding constraint in EM is trivial: just do unconstrained EM in each iteration and project the value to the constraint space (i.e. add constraint) at the end of each iteration. But this doesn't work for MCMC, since we are sampling the distribution but not just the single optimal value.

\textbf{I guess similar problems may happen in other LDS/ GPFA research.}

Even if they make enough constraints to ensure uniqueness, these research should be careful about the pattern they found. For example, in \href{https://papers.nips.cc/paper/2020/hash/aa1f5f73327ba40d47ebce155e785aaf-Abstract.html}{Joshua et al., 2020}, the changes/ different states of interaction make sense to me, but I'm quite skeptical about their results in the dynamic matrix $\mathbf{A}$ itself. Maybe because of block-diagonal structure in loading $\mathbf{C}$, the interaction between population make sense somewhat. But at least the relationship within population doesn't make any sense, because of transformation invariance.

\clearpage


\section{TODO}
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
%	Think about MV-(Poisson)-GLM. Check \href{https://www.tandfonline.com/doi/full/10.1080/03610926.2012.743565?journalCode=lsta20}{ref1} and \href{https://www.tandfonline.com/doi/full/10.1080/02664763.2021.1877637?src=recsys}{ref2}.
	Improve the NUTS.
	\item
	Debug the clustering.
	\item
	Check and clear typo.
%	\item
%	After resolving (1), see if the MCMC is still trapped in local optima sometimes. Now, the chain seems will get stuck in local mode. Maybe updating the loading as a whole will remedy the problem a bit.
	\item
	Implement the mixture of finite mixture (MFM) by Jeff Miller.
	\item
	Find a more efficient way to generate new cluster parameters, otherwise the newly generated ones will always be rejected.
%	\item
%	improve DPMM. In current brute force implementation, the number of potential clusters can even go beyond number of neurons (\(N\)). There are several improvements, e.g. \href{https://link.springer.com/article/10.1007/s11222-009-9150-y}{Kalli et al., 2011}, \href{http://proceedings.mlr.press/v37/gea15.html}{Ge et al., 2015} and \href{https://link.springer.com/article/10.1007/s11222-014-9471-3}{Hastie et al, 2015}. Check them later.
	\item
	After all of them are resolved, switch to GP version
	
\end{enumerate}


\end{document}






