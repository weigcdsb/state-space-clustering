
% load packages
\documentclass[]{article}
\usepackage{amsmath,amssymb}
\usepackage{bm}
\usepackage{lmodern}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{url}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{subfigure}

\title{Model-based Clustering for Neural Populations}
\author{}
\date{}
\begin{document}
\maketitle

The goal for this research is to do model-based clustering for  neural populations, by making use of features for each counting process observation. 

\section{Notations}

Assume we can observe neural activities for \(N\) neurons, with counting observation up to \(T\) steps. Therefore, the observation is a \textsl{N-by-T} matrix, \(\mathbf{Y} \in \mathbb{Z}_{\geq 0}^{N \times T}\) ,with each row represents the recording from single neuron. Denote the recording for neuron \(i\) as
\(\mathbf{y}_{i} = (y_{i1},\ldots,y_{\text{iT}})'\), \(i = 1,\ldots N.\), with the cluster index for neuron \(i\) as
\(z_{i} \in \{ 1,\ldots\}\). The number of neurons in cluster \(j\) is
\(n_{j} = \sum_{i = 1}^{N}{I(z_{i} = j)}\), and
\(\sum_{j = 1,2,\ldots}^{}n_{j} = N\). The proportion/ probability in
cluster \(z_{i}\) is \(\rho_{z_{i}}\).

\section{Clustering Wrapper}
The model-based clustering problem can be transformed into fitting the mixture model (MM). The likelihood for each cluster depends on how we model the counting observation, but fitting strategies for MM are the same for all models. Here, I choose to fit the MM by Gibbs sampler. Depending on whether the number of cluster is finite or not, there are two versions: finite mixture model (FMM) and Dirichlet process mixture model (DPMM).

\subsection{Finite Mixture Model}	
Assume the number of cluster is \(J\). The full likelihood for these \(N\) neurons is
\[L = \prod_{i = 1}^{N}{\rho_{z_{i}}f\left( \mathbf{y}_{i}|\mathbf{\Theta}_{z_{i}} \right)} = \prod_{j = 1}^{J}{\rho_{j}^{n_{j}}\left\lbrack \prod_{i:z_{i} = j}^{}{f\left( \mathbf{y}_{i}|\mathbf{\Theta}_{j} \right)} \right\rbrack}\]
, where \(\mathbf{\Theta}_{j}\) contains all parameters in cluster \(j\) defined by the specific model. Therefore, the parameters need to update are:
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	Cluster indicator: \(\left\{ z_{i} \right\}_{i = 1}^{N}\)
	\item
	Cluster proportion: \(\mathbf{\rho} = (\rho_{1},\ldots\rho_{J})'\)
	\item
	Model parameters: \(\mathbf{\Theta}_{j}\)
\end{enumerate}
The (conditional) priors for clustering-related parameters:
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	Cluster indicator \(\left\{ z_{i} \right\}_{i = 1}^{N}\):
	\(P\left( z_{i} = j \right) = \rho_{j}\)
	\item
	Cluster proportion \(\mathbf{\rho} = (\rho_{1},\ldots\rho_{J})'\):
	\[\mathbf{\rho} \sim Dir(\delta_{1},\ldots\delta_{J})\]
	, where \(\delta_{1} = \ldots = \delta_{J} = 1\)
\end{enumerate}

So, the MCMC( Gibbs sampler) iteration for FMM is:
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	Update \(\left\{ z_{i} \right\}_{i = 1}^{N}\):
	\[P\left( z_{i} = j \middle| \mathbf{y}_{i},\left\{ \mathbf{\Theta}_{j} \right\}_{j = 1}^{J} \right) \propto \rho_{j}f\left( \mathbf{y}_{i}|\mathbf{\Theta}_{j} \right)\]
	\item
	Update \(\mathbf{\rho} = \left( \rho_{1},\ldots\rho_{J} \right)^{'}\):
	\[\mathbf{\rho}|\ \left\{ \mathbf{y}_{i} \right\}_{i = 1}^{N},\ \left\{ z_{i} \right\}_{i = 1}^{N},\left\{ \mathbf{\Theta}_{j} \right\}_{j = 1}^{J} \sim Dir(\delta_{1} + n_{1},\ldots\delta_{J} + n_{J})\]
	\item
	Update \(\mathbf{\Theta}_{j}\): this is defined by the specific model. When there's no
	\(z_{i} = j\), just sample \(\mathbf{\Theta}_{j}\) from priors or by other observation-independent ways.
\end{enumerate}

\subsection{Dirichlet Process Mixture Model}

Since calculation of posterior predictive distribution can be hard or even impossible for complicated models, instead of using the popular CRP representation of DP (Neal, 2020), I choose to use the slice sampler (\href{https://www.tandfonline.com/doi/full/10.1080/03610910601096262}{Walker, 2007}).

Use the ''stick-breaking'' construction for cluster proportion, i.e.
\[\rho_{1} = \eta_{1}\]
\[\rho_{j} = \left( 1 - \eta_{1} \right) \cdot \ldots \cdot \left( 1 - \eta_{j - 1} \right)\eta_{j}\]
\[\eta_{j} \sim Beta(1,\alpha)\]

In the slice sampler for DPMM, the parameters need to update are:
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	``stick-breaking'' elements: \(\eta_{j}\)
	\item
	Augment latent variable: \(\left\{ u_{i} \right\}_{i = 1}^{N}\)
	\item
	Model parameters: \(\mathbf{\Theta}_{j}\)
	\item
	Cluster indicator: \(\left\{ z_{i} \right\}_{i = 1}^{N}\)
\end{enumerate}

So, the MCMC( Gibbs sampler) iteration for DPMM is:
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	update \(\eta_{j}\), for
	\(j = 1,\ldots,{{z^{*} = max}\left\{ z_{i} \right\}}_{i = 1}^{N}\) as
	\[\eta_{j}|\left\{ z_{i} \right\}_{i = 1}^{N},\ldots \sim Beta(n_{j} + 1,\ N - \sum_{l = 1}^{j}n_{l} + \alpha)\]
	\item
	update \(\left\{ u_{i} \right\}_{i = 1}^{N}\):
	\[u_{i}|\mathbf{\rho,\ldots} \sim U(0,\ \rho_{z_{i}})\]
	\item
	update \(\eta_{j}\), for \(j = z^{*} + 1,\ldots,\ s^{*}\). \(s^{*}\)
	is the smallest value, s.t.
	\(\sum_{j = 1}^{s^{*}}\rho_{j} > 1 - \min{\{ u_{1},\ldots,u_{N}\}}\)
	\[\eta_{j} \sim Beta(1,\alpha)\]
	\item
	Update \(\mathbf{\Theta}_{j}\): this is defined by the specific model. When there's no
	\(z_{i} = j\), just sample \(\mathbf{\Theta}_{j}\) from priors or by other observation-independent ways.
	\item
	Update \(\left\{ z_{i} \right\}_{i = 1}^{N}\)
	\[P\left( z_{i} = j \middle| \mathbf{y}_{i},\left\{ \mathbf{\Theta}_{j} \right\},\mathbf{\rho,}\left\{ u_{i} \right\}_{i = 1}^{N} \right) = \frac{f\left( \mathbf{y}_{i}|\mathbf{\Theta}_{j} \right)}{\sum_{j:\rho_{j} > u_{i}}^{}{f\left( \mathbf{y}_{i}|\mathbf{\Theta}_{j} \right)}}\]
\end{enumerate}

\section{linear Dynamical System Model}
Here, I model the observations by a linear dynamical system (LDS) model.

LDS models the multi-dimensional time series using a lower dimensional latent representation of the system, which evolves over time according to linear dynamics. By specifying the linear dynamics and process noise covariance, we can also handle the interactions between different neural populations (clusters).

\subsection{Model Details}
Denote the latent vector in cluster \(j\) as
\(\mathbf{x}_{t}^{(j)} \in \mathbb{R}^{p_{j}}\). For simplicity, assume all \(p_{j} = p\). Each observation follows a Poisson distribution:
\[\log\lambda_{\text{it}} = d_{i} + \mathbf{c'}_{i}\mathbf{x}_{t}^{(z_{i})}\]
\[y_{\text{it}} \sim Poisson(\lambda_{\text{it}})\]
, where \(\mathbf{c}_{i} \in \mathbb{R}^{p}\) and
\(\mathbf{x}_{t}^{(z_{i})} \in \mathbb{R}^{p}\).

Although the loading (\(d_{i}\) and \(\mathbf{c}_{i}\)) is determined by neuron index \(i\), the distribution is also cluster-dependent. That is,
\[\left( d_{i},\mathbf{c}'_{i} \right)' \sim N(\bm{\mu}_{\text{dc}}^{\left( z_{i} \right)},\mathbf{\Sigma}_{\text{dc}}^{(z_{i})})\]
By doing this, the loading within each cluster is also correlated.

Denote all latent states as \(\mathbf{x}_{t} = \left( {\mathbf{x'}_{t}^{(1)}},{\mathbf{x'}_{t}^{(2)}},\ldots \right)'\) and they evolve linearly with a Gaussian noise:
\[\mathbf{x}_{1} \sim N(\mathbf{x}_{0},\mathbf{Q}_{0})\]
\[\mathbf{x}_{t + 1}|\mathbf{x}_{t} \sim N(\mathbf{A}\mathbf{x}_{t} + \mathbf{b},\mathbf{Q})\]
For simplicity, assume \(\mathbf{Q}_{0}\) is known (e.g.
\(\mathbf{Q}_{0} = \mathbf{I} \times 10^{-2}\)).

If we assume process noise covariance is block diagonal (\href{https://papers.nips.cc/paper/2020/hash/aa1f5f73327ba40d47ebce155e785aaf-Abstract.html}{Joshua et al., 2020}), we can write things as:
\[\mathbf{x}_{t + 1}^{(j)}|\mathbf{x}_{t}^{(1)},\mathbf{x}_{t}^{(2)},\ldots \sim N(\sum_{l = 1,\ldots}^{}\mathbf{A}_{j \leftarrow l}\mathbf{x}_{t}^{(l)} + \mathbf{b}_{j},\mathbf{Q}^{(j)})\]

Notice \(\left\{ \mathbf{A}_{j \leftarrow l} \right\}\) forms the full transition matrix as:
\[\mathbf{A} = \ \begin{pmatrix}
	\mathbf{A}_{1 \leftarrow 1} & \mathbf{A}_{1 \leftarrow 2} & \ldots \\
	\mathbf{A}_{2 \leftarrow 1} & \mathbf{A}_{2 \leftarrow 2} & \ldots \\
	\ldots\  & \ldots & \ldots \\
\end{pmatrix}\]
Denote the \(j^{\text{th}}\) row block of \(\mathbf{A}\) as
\(\mathbf{A}_{j} = \begin{pmatrix}
	\mathbf{A}_{j \leftarrow 1} & \mathbf{A}_{j \leftarrow 2} & \ldots \\
\end{pmatrix}\). Then,
\(\sum_{l = 1,\ldots}^{}\mathbf{A}_{j \leftarrow l}\mathbf{x}_{t}^{(l)} + \mathbf{b}_{j}\mathbf{=}\mathbf{A}_{j}\mathbf{x}_{t} + \mathbf{b}_{j}\).

If we further let \(\mathbf{Q}\) be diagonal, with the
\(k^{\text{th}}\) row of \(\mathbf{x}_{t}\), \(\mathbf{A}\),
\(\mathbf{b}\) denoted as \(x_{\text{kt}}\), \(\mathbf{a}_{k}\), \(b_{k}\). The corresponding process noise variance is \(q_{k}\). Then:
\[x_{k,t + 1}|x_{\text{kt}} \sim N\left( \mathbf{a}_{k}^{'}\mathbf{x}_{t} + b_{k},q_{k} \right)\]

To facilitate derivation in Gibbs sampler, write the linear dynamics of
\(\mathbf{x}_{t}\) in MV-GLM form, i.e.

\[\mathbf{x}_{t + 1}^{'} = \begin{pmatrix}
	1 & \mathbf{x}_{t}^{'} \\
\end{pmatrix}\begin{pmatrix}
	\mathbf{b}^{'} \\
	\mathbf{A}^{'} \\
\end{pmatrix} + \mathbf{\epsilon}_{i}^{'}\]

, where \(\mathbf{\epsilon} \sim N(0,\mathbf{Q})\). Then if we stack the
latent by row, the problem is reduced to a multivariate general linear model (MV-GLM) problem:

\[\begin{pmatrix}
	\mathbf{x}_{2}^{'} \\
	\mathbf{x}_{3}^{'} \\
	\vdots \\
	\mathbf{x}_{T}^{'} \\
\end{pmatrix} = \begin{pmatrix}
	1 & \mathbf{x}_{1}^{'} \\
	1 & \mathbf{x}_{2}^{'} \\
	\vdots & \vdots \\
	1 & \mathbf{x}_{T - 1}^{'} \\
\end{pmatrix}\begin{pmatrix}
	\mathbf{b}^{'} \\
	\mathbf{A}^{'} \\
\end{pmatrix} + \mathbf{E}\]

, where
\(\mathbf{E} = \left( \mathbf{\epsilon}_{1},\ldots,\mathbf{\epsilon}_{T - 1} \right)^{'}\).

In summary, Let
\(\mathbf{\Theta} = \left\{ \mathbf{z},\ \mathbf{d},\mathbf{C},\left\{ \mathbf{x}_{t} \right\}_{t = 1}^{T},\mathbf{x}_{0},\mathbf{Q}_{0},\mathbf{A},\ \mathbf{b},\mathbf{Q}\  \right\}\)
be the set of parameters for LDS. The number of observation/ neuron is
\(N\), and they can group into \(J\) clusters. In each cluster, there
are \(p\) latent state vectors. The recording length is \(T\).
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	\(\mathbf{z} \in \mathbb{Z}^{N}\): cluster index for each neuron/ observation,
	with \(z_{i} \in \{ 1,\ldots,J\}\) for \(i = 1,\ldots,N\).
	\item
	\(\mathbf{d} \in \mathbb{R}^{N}\) and \(\mathbf{C} \in \mathbb{R}^{N \times p}\): baseline
	\& loading of the latent
	\item
	\(\mathbf{x}_{t} \in \mathbb{R}^{Jp}\): all latent at \(t\).
	\item
	\(\mathbf{x}_{t}^{(j)} \in \mathbb{R}^{p}\) means the latent in cluster \(j\) at
	time \(t\).
	\item
	\(\mathbf{x}_{0} \in \mathbb{R}^{Jp}\) and
	\(\mathbf{Q}_{0} \in \mathbb{R}^{Jp \times Jp}\): mean and covariance of
	\(\mathbf{x}_{1}\). For simplicity, assume \(\mathbf{Q}_{0}\) is known.
	\item
	\(\mathbf{A} \in \mathbb{R}^{Jp \times Jp},\ \mathbf{b} \in \mathbb{R}^{Jp},\mathbf{Q} \in \mathbb{R}^{Jp \times Jp}\):
	linear dynamics of the latent.
\end{enumerate}

Since the progress noise is independent at each step and the observation is assumed conditional independent, the likelihood is:
\[ f(\mathbf{y}|\mathbf{\Theta})= \prod_{i = 1}^{N}\prod_{t = 1}^{T}{P(y_{it}|\mathbf{\Theta})} = \prod_{i = 1}^{N}\prod_{t = 1}^{T}{POI(y_{it}|\exp{(d_{i} + \mathbf{c'}_{i}\mathbf{x}_{t}^{(z_{i})})})}
\]
, where \(POI(\cdot|\lambda)\) is the density of \(Poisson(\lambda)\).

\textbf{DETOUR}:\\
Maybe in the future, we may switch to EM. To help with this, I also give the likelihood for complete observation \((\mathbf{y}, \left\{ \mathbf{x}_{t} \right\}_{t = 1}^{T})\). Let \(\mathbf{\Theta}'=\mathbf{\Theta}\backslash \left\{ \mathbf{x}_{t} \right\}_{t = 1}^{T}\).
\[ f(\mathbf{y}, \left\{ \mathbf{x}_{t} \right\}_{t = 1}^{T}|\mathbf{\Theta}')= \prod_{i = 1}^{N}\prod_{t = 1}^{T}{P(y_{it}, \mathbf{x}_{t}|\mathbf{\Theta}')} = \prod_{i = 1}^{N}\prod_{t = 1}^{T}{POI(y_{it}|\exp{(d_{i} + \mathbf{c'}_{i}\mathbf{x}_{t}^{(z_{i})})})}\cdot N(\mathbf{x}_{t}^{(z_{i})} |\mathbf{A}_{z_i}\mathbf{x}_{t} + \mathbf{b}_{z_i}, \mathbf{Q}_{z_i})
\]
, where \(\mathbf{A}_j\in \mathbb{R}^{p \times Jp}\) and \(\mathbf{b}_j\in \mathbb{R}^p\)  are the \(j^{\text{th}}\) row block of \(\mathbf{A}\) and \(\mathbf{b}\). \(\mathbf{Q}_j \in \mathbb{R}^{p \times p}\) is the \(j^{\text{th}}\) row and column block of \(\mathbf{Q}\).

\subsection{Conditional Priors for Parameters}
The parameters need to estimate:
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	Latent vectors: \(\left\{ \mathbf{x}_{t} \right\}_{t=1}^T\)
	\item
	Initials: \(\mathbf{x}_{0}\)
	\item
	Linear mapping (loading) for latent vectors:
	\(\left\{ d_{i} \right\}_{i = 1}^{N}\) and
	\(\left\{ \mathbf{c}_{i} \right\}_{i = 1}^{N}\)
	\item
	Mean and covariance for loading in each cluster:
	\(\left\{ \bm{\mu}_{\text{dc}}^{(j)} \right\}_{j}\) and
	\(\left\{ \mathbf{\Sigma}_{\text{dc}}^{(j)} \right\}_{j}\)
	\item
	Linear dynamics for latent vectors: \(\mathbf{A}\) and \(\mathbf{b}\)
	\item
	Process noise: \(\mathbf{Q}\)
\end{enumerate}

The conditional priors for these parameters:
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	Latent vectors \(\left\{ \mathbf{x}_{t} \right\}_{t=1}^T\): the conditional prior is
	defined by
	\[\mathbf{x}_{1} \sim N(\mathbf{x}_{0},\mathbf{Q}_{0})\]
	\[\mathbf{x}_{t + 1}|\mathbf{x}_{t} \sim N(\mathbf{A}\mathbf{x}_{t} + \mathbf{b},\mathbf{Q})\]
	\item
	Initials \(\mathbf{x}_{0}\): assume there are \(J\) clusters,
	\[\mathbf{x}_{0} \sim N(\bm{\mu}_{\mathbf{x}_{00}},\ \mathbf{\Sigma}_{\mathbf{x}_{00}})\]
	, where \(\bm{\mu}_{\mathbf{x}_{00}} = \mathbf{0}_{Jp}\) and \(\mathbf{\Sigma}_{\mathbf{x}_{00}} = \mathbf{I}_{Jp}\)
	\item
	Linear mapping (loading) for latent vectors
	\(\left\{ d_{i} \right\}_{i = 1}^{N}\) and
	\(\left\{ \mathbf{c}_{i} \right\}_{i = 1}^{N}\):
	\[\left( d_{i},\mathbf{c}'_{i} \right)' \sim N(\bm{\mu}_{\text{dc}}^{\left( z_{i} \right)},\mathbf{\Sigma}_{\text{dc}}^{(z_{i})})\]
	\item
	Mean and covariance for loading in each cluster
	Mean and covariance for loading in each cluster
	\(\left\{ \bm{\mu}_{\text{dc}}^{(j)} \right\}_{j}\) and
	\(\left\{ \mathbf{\Sigma}_{\text{dc}}^{(j)} \right\}_{j}\):
	\[\bm{\mu}_{\text{dc}}^{(j)} \sim N(\mathbf{\delta}_{dc0},\mathbf{T}_{dc0})\]
	, where \(\bm{\delta}_{dc0} = \mathbf{0}_{p + 1}\) and
	\(\mathbf{T}_{dc0} = \mathbf{I}_{p + 1}\)
	
	\[\mathbf{\Sigma}_{\text{dc}}^{(j)} \sim W^{- 1}\left( \Psi_{dc0},\nu_{dc0} \right)\]
	, where \(\nu_{dc0} = p + 1 + 2\) and
	\(\Psi_{dc0} = \mathbf{I}_{p + 1} \times 10^{-4}\)
	
	\item
	Linear dynamics for latent vectors: \(\mathbf{A}\), \(\mathbf{b}\) and process noise \(\mathbf{Q}\).\\ 
	Denote \(\mathbf{F} = \begin{pmatrix}
		\mathbf{b}^{'} \\
		\mathbf{A}^{'} \\
	\end{pmatrix}\), \(\mathbf{f} = vec(\mathbf{F})\)
	
	\begin{align*}
		P\left( \mathbf{f},\ \mathbf{Q} \right) &= P(\mathbf{Q})P(\mathbf{f}|\mathbf{Q})\\
		\mathbf{Q} &\sim W^{- 1}(\Psi_{\mathbf{Q}_{0}},\nu_{\mathbf{Q}_{0}})\\
		\mathbf{f} &\sim N(\mathbf{f}_0, \mathbf{Q}\bigotimes \bm{\Lambda}_0^{-1})
	\end{align*}
	
	, where \(\mathbf{f}_0 = vec(\mathbf{F}_0)\).If the number of cluster is \(J\), \(\nu_{\mathbf{Q}_{0}} = Jp + 2\),
	\(\Psi_{\mathbf{Q}_{0}} = \mathbf{I}_{Jp} \times 10^{- 4}\). (To make the mean of \(\mathbf{Q}\) loosely centered around
	\(\mathbf{I}_{Jp} \times 10^{- 4}\)), \(\mathbf{F}_0 = \begin{pmatrix}
		\mathbf{0}'_{Jp} \\
		\mathbf{I}_{Jp} \\
	\end{pmatrix}\) and \(\bm{\Lambda}_0 = \mathbf{I}_{Jp + 1}\)
	
\end{enumerate}

\subsection{MCMC (Gibbs Sampler)}

\subsubsection{Update \(\left\{ \mathbf{x}_{t} \right\}_{t=1}^T\)}
Use Laplace approximation and make use of the block tri-diagonal Hessian.

Denote \(t^{\text{th}}\) column of mean firing rate and observation as \({\widetilde{\bm{\lambda}}}_{t} = \left( \lambda_{1t},\ldots,\lambda_{\text{Nt}} \right)'\)
and \({\widetilde{\mathbf{y}}}_{t} = (y_{1t},\ldots,y_{\text{Nt}})'\).
The linear mapping matrix for all observations is \(\mathbf{C}\), such that
\(\log{\widetilde{\bm{\lambda}}}_{t} = \mathbf{d} + \mathbf{C}\mathbf{x}_{t}\).
Let \(\mathbf{x} = \left( \mathbf{x'}_{1},\ldots,\mathbf{x'}_{T} \right)'\)and
\(f\left( \mathbf{x} \right) = \log{P(\mathbf{x}|\left\{ \mathbf{y}_{i} \right\}_{i = 1}^{N},\mathbf{C},\mathbf{Q}_{0},\mathbf{A},\mathbf{b},\mathbf{Q},\ldots)}\)
The first and second derivative with respect to \(\mathbf{x}\), for \(t=2, \ldots, T-1\):
\begin{align*}
	\frac{\partial f}{\partial\mathbf{x}_{1}} &= \mathbf{C'}\left( {\widetilde{\mathbf{y}}}_{1} - {\widetilde{\bm{\lambda}}}_{1} \right) - \mathbf{Q}_{0}^{- 1}\left( \mathbf{x}_{1} - \mathbf{x}_{0} \right) + \mathbf{A}'\mathbf{Q}^{- 1}\mathbf{(}\mathbf{x}_{2} - \mathbf{A}\mathbf{x}_{1} - \mathbf{b)} \\
	\frac{\partial f}{\partial\mathbf{x}_{t}} &= \mathbf{C'}\left( {\widetilde{\mathbf{y}}}_{t} - {\widetilde{\bm{\lambda}}}_{t} \right) - \mathbf{Q}^{- 1}\left( \mathbf{x}_{t} - \mathbf{A}\mathbf{x}_{t - 1} - \mathbf{b} \right) + \mathbf{A}'\mathbf{Q}^{- 1}(\mathbf{x}_{t + 1} - \mathbf{A}\mathbf{x}_{t} -\mathbf{b)}\\
	\frac{\partial f}{\partial\mathbf{x}_{T}} &= \mathbf{C'}\left( {\widetilde{\mathbf{y}}}_{T} - {\widetilde{\bm{\lambda}}}_{T} \right) - \mathbf{Q}^{- 1}\left( \mathbf{x}_{T} - \mathbf{A}\mathbf{x}_{T - 1} - \mathbf{b} \right)\\
	\frac{\partial^{2}f}{\partial\mathbf{x}_{1}\partial\mathbf{x}'_{1}} &= - \mathbf{C'}\text{Diag}\left( {\widetilde{\bm{\lambda}}}_{1} \right)\mathbf{C} - \mathbf{Q}_{0}^{- 1} - \mathbf{A}'\mathbf{Q}^{- 1}\mathbf{A}\\
	\frac{\partial^{2}f}{\partial\mathbf{x}_{t}\partial\mathbf{x}'_{t}} &= - \mathbf{C'}\text{Diag}\left( {\widetilde{\bm{\lambda}}}_{t} \right)\mathbf{C} - \mathbf{Q}^{- 1} - \mathbf{A}'\mathbf{Q}^{- 1}\mathbf{A}\\
	\frac{\partial^{2}f}{\partial\mathbf{x}_{T}\partial\mathbf{x}'_{T}} &= - \mathbf{C'}\text{Diag}\left( {\widetilde{\bm{\lambda}}}_{T} \right)\mathbf{C} - \mathbf{Q}^{- 1}\\
	\frac{\partial^{2}f}{\partial\mathbf{x}_{1}\partial\mathbf{x}'_{2}} &= \frac{\partial^{2}f}{\partial\mathbf{x}_{t}\partial\mathbf{x}'_{t + 1}} = \mathbf{A}'\mathbf{Q}^{- 1}
	&&\frac{\partial^{2}f}{\partial\mathbf{x}_{t}\partial\mathbf{x}'_{t - 1}} = \mathbf{Q}^{- 1}\mathbf{A}
\end{align*}

So, the gradient is:

\[\nabla = \frac{\partial f}{\partial\mathbf{x}} = \left( \left( \frac{\partial f}{\partial\mathbf{x}_{1}} \right)',\ \ldots,\left( \frac{\partial f}{\partial\mathbf{x}_{T}} \right)' \right)'\]

And the block tri-diagonal Hessian:

\[H = \frac{\partial^{2}f}{\partial\mathbf{x}\partial\mathbf{x}^{'}} = \begin{pmatrix}
	\frac{\partial^{2}f}{\partial\mathbf{x}_{1}\partial\mathbf{x}'_{1}} & \mathbf{A'}\mathbf{Q}^{- 1} & 0 & \cdots & 0 \\
	\mathbf{Q}^{- 1}\mathbf{A} & \frac{\partial^{2}f}{\partial\mathbf{x}_{2}\partial\mathbf{x}'_{2}} & \mathbf{A'}\mathbf{Q}^{- 1} & \cdots & \vdots \\
	0 & \mathbf{Q}^{- 1}\mathbf{A} & \frac{\partial^{2}f}{\partial\mathbf{x}_{3}\partial\mathbf{x}'_{3}} & \cdots & \vdots \\
	\vdots & \vdots & \vdots & \ddots & \vdots \\
	0 & \cdots & \cdots & \cdots & \frac{\partial^{2}f}{\partial\mathbf{x}_{T}\partial\mathbf{x}'_{T}} \\
\end{pmatrix}\]

Use Newton-Raphson to find
\(\bm{\mu}_{\mathbf{x}} =\text{argmax}_{\mathbf{x}}\left( f\left( \mathbf{x} \right) \right)\)
and \(\mathbf{\Sigma}_{\mathbf{x}}= -\left\lbrack \frac{\partial^{2}f}{\partial\mathbf{x}\partial\mathbf{x}'}\left. \ \mathbf{} \right|_{\mathbf{X} = \bm{\mu}_{\mathbf{X}}} \right\rbrack^{\mathbf{- 1}}\), such that \((P(\mathbf{x}|\left\{ \mathbf{y}_{i} \right\}_{i = 1}^{N},\ldots)\ \approx N\left(\bm{\mu}_{\mathbf{x}}, \mathbf{\Sigma}_{\mathbf{x}}\right)\).
When using Newton-Raphson (NR), \(H\backslash\nabla\) in MATLAB will make use
of block tri-diagonal structure automatically.

However, NR is not robust to bad initials. At the first few iterations, simply using fitting from previous step may lead to infinite Hessian. When the initial from previous step fails, use the approximation at recursive priors, , i.e. the adaptive smoother estimates, as the initial. The adaptive smoother estimates are from backward RTS smoother from adaptive filter, and the details about Poisson adaptive filter can be found in  \href{http://www.stat.columbia.edu/~liam/teaching/neurostat-spr11/papers/brown-et-al/eden2004.pdf}{Eden et al., 2004}.

To sample efficiently and make best use of sparse covariance, use Cholesky decomposition of
\(\mathbf{\Sigma}_{\mathbf{x}}^{- 1} = \mathbf{R}\mathbf{R}'\):
sample
\(\mathbf{Z} \sim N(\mathbf{R}'\mathbf{\mu}_{\mathbf{x}},\mathbf{I})\),
then
\(\mathbf{x} = \left( \mathbf{R}' \right)^{- 1}\mathbf{Z} \sim N(\bm{\mu}_{\mathbf{x}},\mathbf{\Sigma}_{\mathbf{x}})\).   

\textbf{Step Further:}\\
Using normal approximation may not be accurate enough, and this may lead to a bad mixing. We can step further to use the Metropolis-Hasting, based on the normal approximated variance. To be specific, the proposal distribution is \(Q(\mathbf{x}^{*}|\mathbf{x}^{(s)}) \sim N(\mathbf{x}^{(s)}, \alpha\mathbf{\Sigma}_{\mathbf{x}})\), where \(\alpha\) is a scalar to make the acceptance rate to be 0.4 to 0.6. According to \href{https://projecteuclid.org/journals/statistical-science/volume-16/issue-4/Optimal-scaling-for-various-Metropolis-Hastings-algorithms/10.1214/ss/1015346320.full}{Roberts and Rosenthal, 2001}. For high dimensional MH, the optimal proposal is \(N(x, 2.38^2\Sigma/d)\). See details \& experiments in Section \ref{sim}: Simulations.


\subsubsection{Update \(\mathbf{x}_{0}\)}
\[P\left( \mathbf{x}_{0}|\mathbf{x}_{1},\ \mathbf{Q}_{0}\ldots \right) \propto N(\mathbf{x}_{1}|\mathbf{x}_{0},\ \mathbf{Q}_{0})N(\mathbf{x}_{0}|\bm{\mu}_{\mathbf{x}_{00}},\ \mathbf{\Sigma}_{\mathbf{x}_{00}})\]
By conjugacy, \(\mathbf{x}_{0}|\mathbf{x}_{1},\ \mathbf{Q}_{0}\ldots \sim N(\bm{\mu}_{\mathbf{x}_{0}},\ \mathbf{\Sigma}_{\mathbf{x}_{0}})\)
\begin{align*}
	\mathbf{\Sigma}_{\mathbf{x}_{0}} &= \left\lbrack \mathbf{\Sigma}_{\mathbf{x}_{00}}^{- 1} + \mathbf{Q}_{0}^{- 1} \right\rbrack^{- 1}\\
	\bm{\mu}_{\mathbf{x}_{0}} &= \mathbf{\Sigma}_{\mathbf{x}_{0}}\left( \mathbf{\Sigma}_{\mathbf{x}_{00}}^{- 1}\bm{\mu}_{\mathbf{x}_{00}} + \mathbf{Q}_{0}^{- 1}\mathbf{x}_{1} \right)
\end{align*}

\subsubsection{Update \(\left\{ d_{i} \right\}_{i = 1}^{N}\) and
	\(\left\{ \mathbf{c}_{i} \right\}_{i = 1}^{N}\)}
To update efficiently, use Laplace approximation and the error is independent for each row (i.e. update things row by row). Denote
\(\left( d_{i},\mathbf{c}'_{i} \right)' = \bm{\zeta}_{i} \in \mathbb{R}^{p + 1}\)
and \(\left( 1,{\mathbf{x}_{t}'^{\left( z_{i} \right)}} \right) = {{\widetilde{\mathbf{x}}}_{t}'^{\left( z_{i} \right)}}\).
\begin{align*}
	P\left( \bm{\zeta}_{i} \middle| \mathbf{y}_{i},\ \left\{ \mathbf{x}_{t}^{\left( z_{i} \right)} \right\}_{t = 1}^{T},\ldots \right) &= \exp{f\left( \bm{\zeta}_{i} \right)} \approx N\left( \bm{\zeta}_{i}|\bm{\mu}_{\bm{\zeta}_{i}},\mathbf{\Sigma}_{\bm{\zeta}_{i}}\  \right)\\
	\frac{\partial f}{\partial\bm{\zeta}_{i}} &= \frac{\partial l}{\partial\bm{\zeta}_{i}} - {\mathbf{\Sigma}_{\text{dc}}^{\left( z_{i} \right)}}^{- 1}\left( \bm{\zeta}_{i} - \bm{\mu}_{\text{dc}}^{(z_{i})} \right) = \left\lbrack \sum_{t = 1}^{T}{\widetilde{\mathbf{x}}}_{t}^{\left( z_{i} \right)}\left( y_{\text{it}} - \lambda_{\text{it}} \right) \right\rbrack - {\mathbf{\Sigma}_{\text{dc}}^{\left( z_{i} \right)}}^{- 1}\left( \bm{\zeta}_{i} - \bm{\mu}_{\text{dc}}^{(z_{i})} \right)\\
	\frac{\partial^{2}f}{\partial\bm{\zeta}_{i}\partial\bm{\zeta}'_{i}} &= \frac{\partial^{2}l}{\partial\bm{\zeta}_{i}\partial\bm{\zeta}'_{i}} - {\mathbf{\Sigma}_{\text{dc}}^{\left( z_{i} \right)}}^{- 1} = -\left\lbrack \sum_{t = 1}^{T}{\lambda_{\text{it}}{\widetilde{\mathbf{x}}}_{t}^{\left( z_{i} \right)}{{\widetilde{\mathbf{x}}}_{t}'^{\left( z_{i} \right)}}} \right\rbrack - {\mathbf{\Sigma}_{\text{dc}}^{\left( z_{i} \right)}}^{- 1}
\end{align*}
, where \(l\) is Poisson log-likelihood. Then use Newton-Raphson to find
\(\bm{\mu}_{\bm{\zeta}_{i}} = \text{argmax}_{\bm{\zeta}_{i}}\left( f\left( \bm{\zeta}_{i} \right) \right)\)
and \(\mathbf{\Sigma}_{\bm{\zeta}_{i}} = -\left\lbrack \frac{\partial^{2}f}{\partial\bm{\zeta}_{i}\partial\bm{\zeta}'_{i}}\left.  \right|_{\bm{\zeta}_{i} = \bm{\mu}_{\bm{\zeta}_{i}}} \right\rbrack^{-1}\). If initial as the previous step fits fails, simply use prior mean of \(\bm{\mu}_{\bm{\zeta}_{i}}\), i.e. \(\bm{\delta}_{dc0}\).


%DETOUR: we can write things in the form of MV-(Poisson)-GLM as follows (in cluster j with \(n_j\) observations/ neurons, with corresponding loading be \(\mathbf{d}_{j}\) and \(\mathbf{C}_{j}\)):
%
%\[\log\begin{pmatrix}
%	\lambda_{11} & \cdots & \lambda_{1n_{j}} \\
%	\vdots & \ddots & \vdots \\
%	\lambda_{T1} & \cdots & \lambda_{Tn_{j}} \\
%\end{pmatrix} = \begin{pmatrix}
%	1 & \mathbf{x}_{1}^{'} \\
%	\vdots & \vdots \\
%	1 & \mathbf{x}_{T}^{'} \\
%\end{pmatrix}\begin{pmatrix}
%	\mathbf{d}_{j}^{'} \\
%	\mathbf{C}_{j}^{'} \\
%\end{pmatrix} + \mathbf{E}_j\]
%
%We can further organize things column-wise. let \(\begin{pmatrix}
%	1 & \mathbf{x}_{1}^{'} \\
%	\vdots & \vdots \\
%	1 & \mathbf{x}_{T}^{'} \\
%\end{pmatrix} = \mathbf{G}\), then the design matrix and corresponding log-mean response are \(\begin{pmatrix}
%\mathbf{G} & \mathbf{0} & \cdots & \mathbf{0} \\
%\mathbf{0} & \mathbf{G} & \cdots & \mathbf{0} \\
%\vdots & \vdots & \ddots & \vdots \\
%\mathbf{0} & \cdots & \cdots & \mathbf{G} \\
%\end{pmatrix}\) and \({\log\begin{pmatrix}
%	\lambda_{11}^{(j)} & \cdots & \lambda_{T1}^{(j)} & \lambda_{12}^{(j)} & \cdots & \lambda_{T2}^{(j)} & \cdots & \lambda_{Tn_{j}}^{(j)} \\
%\end{pmatrix}}'\).
%
%However, when I use the Laplace approximation and NR, because of the block-diagonal structure of design matrix, the error is always independent observation by observation. This is equivalent to observation-wise update as above.
%
%\textbf{Maybe I did something wrong?}
%The point to say this is to make loading within each cluster depends on each other. Check these two later(\href{https://www.tandfonline.com/doi/full/10.1080/03610926.2012.743565?journalCode=lsta20}{ref1} and \href{https://www.tandfonline.com/doi/full/10.1080/02664763.2021.1877637?src=recsys}{ref2}).
%
%Now I do it in another way: make the prior cluster-dependent...


\textbf{Step further}:
Again, use MH to sample the posterior, based on the normal approximated variance. Since the dimension of d and C is not large (just d = 3) in the simulation, I didn't use the optimal scalar yet. \textbf{Modify it later}.

\subsubsection{Update \(\left\{ \mathbf{\mu}_{\text{dc}}^{(j)} \right\}_{j}\) and \(\left\{ \mathbf{\Sigma}_{\text{dc}}^{(j)} \right\}_{j}\)} \label{loading prior}

Purpose: to make loading within each cluster depends on each other, and this will help with clustering. As above, denote \(\left( d_{i},\mathbf{c}'_{i} \right)' = \bm{\zeta}_{i} \in \mathbb{R}^{p + 1}\).
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	Mean \(\left\{ \bm{\mu}_{\text{dc}}^{(j)} \right\}_{j}\): by
	conjugacy, \(\bm{\mu}_{\text{dc}}^{(j)} \sim N\left( \bm{\delta}_{\text{dc}},\mathbf{T}_{\text{dc}}\  \right)\)
	\begin{align*}
		\mathbf{T}_{\text{dc}}^{- 1} &= \left( \mathbf{T}_{dc0}^{- 1} + n_{j}{\mathbf{\Sigma}_{\text{dc}}^{(j)}}^{- 1} \right)^{- 1}\\
		\bm{\delta}_{\text{dc}} &= \mathbf{T}_{\text{dc}}\left( \mathbf{T}_{dc0}^{- 1}\bm{\delta}_{dc0} + {\mathbf{\Sigma}_{\text{dc}}^{(j)}}^{- 1}\sum_{i:z_{i} = j}^{}\bm{\zeta}_{i} \right)
	\end{align*}
	\item
	Covariance \(\left\{ \mathbf{\Sigma}_{\text{dc}}^{(j)} \right\}_{j}\): by conjugacy, \(\mathbf{\Sigma}_{\text{dc}}^{(j)} \sim W^{- 1}\left( \Psi_{\text{dc}},\nu_{\text{dc}} \right)\)
	\begin{align*}
		\nu_{\text{dc}} &= n_{j} + \nu_{dc0}\\
		\Psi_{\text{dc}} &= \Psi_{dc0} + \sum_{i:z_{i} = j}^{}{\left( \bm{\zeta}_{i} - \bm{\mu}_{\text{dc}}^{(j)} \right)\left( \bm{\zeta}_{i} - \bm{\mu}_{\text{dc}}^{(j)} \right)'}
	\end{align*}
\end{enumerate}

\subsubsection{Update \(\mathbf{A}\), \(\mathbf{b}\) and \(\mathbf{Q}\)} 
Here, I only give the update for full \(\mathbf{Q}\). If we want to assume block-diagonal or diagonal structure of \(\mathbf{Q}\), just update things cluster-by-cluster or latent-by-latent.

As shown before, the problem is the usual Bayesian MV-GLM problem. Denote \(\mathbf{F} = \begin{pmatrix}
	\mathbf{b}^{'} \\
	\mathbf{A}^{'} \\
\end{pmatrix}\), \(\mathbf{f} = vec(\mathbf{F})\), \(\mathbf{Y}_\mathbf{bA} = (\mathbf{x}_{2}, \mathbf{x}_{3} \ldots, \mathbf{x}_{T})'\) and \(\mathbf{X}_\mathbf{bA} = \begin{pmatrix}
1 & \mathbf{x}_{1}^{'} \\
1 & \mathbf{x}_{2}^{'} \\
\vdots & \vdots \\
1 & \mathbf{x}_{T - 1}^{'} \\
\end{pmatrix}\)

\[\mathbf{Y}_\mathbf{bA} = \mathbf{X}_\mathbf{bA}\mathbf{F}+ \mathbf{E}\]

, where
\(\mathbf{E} = \left( \mathbf{\epsilon}_{1},\ldots,\mathbf{\epsilon}_{T - 1} \right)^{'}\) and \(\mathbf{\epsilon} \sim N(0,\mathbf{Q})\). Then posteriors are
\begin{align*}
	\mathbf{Q}|\mathbf{Y}_{\mathbf{bA}},\mathbf{X}_{\mathbf{bA}},\ldots &\sim W^{-1}(\Psi_{\mathbf{Q}}, \nu_{\mathbf{Q}})\\
	\mathbf{f}|\mathbf{Y}_{\mathbf{bA}},\mathbf{X}_{\mathbf{bA}},\mathbf{Q},\ldots &\sim N(vec(\mathbf{F}_\mathbf{bA}),  \mathbf{Q}\bigotimes \bm{\Lambda}_\mathbf{bA}^{-1})
\end{align*}
, with parameters be:
\begin{align*}
	\Psi_{\mathbf{Q}} &= \Psi_{\mathbf{Q}_{0}} + (\mathbf{Y}_\mathbf{bA} - \mathbf{X}_\mathbf{bA}\mathbf{F}_\mathbf{bA})'(\mathbf{Y}_\mathbf{bA} - \mathbf{X}_\mathbf{bA}\mathbf{F}_\mathbf{bA}) + (\mathbf{F}_\mathbf{bA} - \mathbf{F}_0)'\bm{\Lambda}_0(\mathbf{F}_\mathbf{bA} - \mathbf{F}_0)\\
	\nu_{\mathbf{Q}} &= \nu_{\mathbf{Q}_{0}} + T - 1\\
	\mathbf{F}_\mathbf{bA} &= (\mathbf{X}'_\mathbf{bA}\mathbf{X}_\mathbf{bA} + \bm{\Lambda}_0)^{-1}(\mathbf{X}'_\mathbf{bA}\mathbf{Y}_\mathbf{bA} + \bm{\Lambda}_0\mathbf{F}_0)\\
	\bm{\Lambda}_\mathbf{bA} &= \mathbf{X}'_\mathbf{bA}\mathbf{X}_\mathbf{bA} + \bm{\Lambda}_0
\end{align*}




\section{Simulations}\label{sim}

\textbf{\textcolor{red}{All the simulation results are old and problematic. Update them later.}}

\section{Problem from Affine Transformation}
Let me review the model first.\\
Assume there are \(J\)
clusters, with \(n\) neurons in each cluster. Each cluster is governed
by \(p\) latent vectors. \(\bm{\lambda}_{t}^{(j)} \in \mathbb{R}^{n}\),
\(\mathbf{x}_{t}^{(j)} \in \mathbb{R}^{p}\), \(\mathbf{d}^{(j)} \in \mathbb{R}^{n}\) and
\(\mathbf{C}^{(j)} \in \mathbb{R}^{n \times p}\), with \(j = 1,\ldots,J\), be the
mean firing rate, latent state, intercept and loading (at time $t$) for
each cluster. Then I just stack all the thing together, and denote
corresponding things as \(\bm{\lambda}_{t} \in R^{\text{Jn}}\),
\(\mathbf{x}_{t} \in \mathbb{R}^{Jp}\), \(\mathbf{d} \in \mathbb{R}^{\text{Jn}}\)
and \(\mathbf{C} \in \mathbb{R}^{Jn \times Jp}\). Notice \(\mathbf{C}\) is block
diagonal.

\[\log{\begin{pmatrix}
		\ \bm{\lambda}_{t}^{(1)} \\
		\vdots \\
		\ \bm{\lambda}_{t}^{(J)} \\
	\end{pmatrix} =}\log\bm{\lambda}_{t} = \begin{pmatrix}
	\ \mathbf{d}^{(1)} \\
	\vdots \\
	\ \mathbf{d}^{(J)} \\
\end{pmatrix} + \begin{pmatrix}
	\mathbf{C}^{(1)} & \cdots & 0 \\
	\vdots & \ddots & \vdots \\
	0 & \cdots & \mathbf{C}^{(J)} \\
\end{pmatrix}\begin{pmatrix}
	\ \mathbf{x}_{t}^{(1)} \\
	\vdots \\
	\ \mathbf{x}_{t}^{(J)} \\
\end{pmatrix} = \mathbf{d} + \mathbf{C}\mathbf{x}_{t}\]

The latent states progress linearly with a Gaussian noise:

\[\mathbf{x}_{t + 1}|\mathbf{x}_{t} \sim N(\mathbf{A}\mathbf{x}_{t} + \mathbf{b},\ \mathbf{Q})\]

, where \(\mathbf{A} \in \mathbb{R}^{Jp \times Jp}\),
\(\mathbf{b} \in \mathbb{R}^{Jp}\) and
\(\mathbf{Q} \in \mathbb{R}^{Jp \times Jp}\).

Now assume we take an affine transformation of latent states as
\(\mathbf{x}_{t}^{*} = \mathbf{M}\mathbf{x}_{t} + \mathbf{g}\). To
preserve the block diagonal structure of \(\mathbf{C}\), \(\mathbf{M}\)
is also block diagonal. Then,
\begin{align*}
	\log\bm{\lambda}_{t} &= \mathbf{d} + \mathbf{C}\mathbf{M}^{- 1}\left( \mathbf{M}\mathbf{x}_{t} + \mathbf{g} \right) - \mathbf{C}\mathbf{M}^{- 1}\mathbf{g}\\
	&= \left( \mathbf{d} - \mathbf{C}\mathbf{M}^{- 1}\mathbf{g} \right) + \mathbf{C}\mathbf{M}^{- 1}(\mathbf{M}\mathbf{x}_{t} + \mathbf{g})\\
	\mathbf{M}\mathbf{x}_{t + 1} + \mathbf{g}|\mathbf{x}_{t} &\sim N_{Jp}\left( \mathbf{M}\left( \mathbf{A}\mathbf{x}_{t} + \mathbf{b} \right)+\mathbf{g}, \mathbf{MQM}' \right)\\
	&= N_{Jp}(\mathbf{MAM}^{- 1}\left( \mathbf{M}\mathbf{x}_{t} + \mathbf{g} \right) + \left( \mathbf{Mb} + \mathbf{g} - \mathbf{MAM}^{- 1}\mathbf{g} \right), \mathbf{MQM}')
\end{align*}

That means, when \(\left\{ \mathbf{d},\mathbf{C},\mathbf{x}_t,\mathbf{A},\mathbf{b},\mathbf{Q} \right\}\) is a solution,
\(\left\{ \mathbf{d}^*,\mathbf{C}^*,\mathbf{x}^*_t,\mathbf{A}^*,\mathbf{b}^*,\mathbf{Q}^* \right\}\) is also a
solution, where
\(\mathbf{d}^* = \mathbf{d} - \mathbf{C}\mathbf{M}^{- 1}\mathbf{g}\),
\(\mathbf{C}^* = \mathbf{CM}^{- 1}\), \(\mathbf{x}_{t}^{*} = \mathbf{M}\mathbf{x}_{t} + \mathbf{g}\),
\(\mathbf{A}^* = \mathbf{MAM}^{- 1}\),
\(\mathbf{b}^* = \mathbf{Mb} + \mathbf{g} - \mathbf{MAM}^{- 1}\mathbf{g}\)
and \(\mathbf{Q}^* = \mathbf{MQM}'\), for any block diagonal \(\mathbf{M}\). Therefore, we must add some constraints to ensure the unique solution/ stationary
posterior!

\subsection{Constraints}

%\subsubsection{constraint on $\bm{x}_t$ only}
\subsubsection{Constraint on latent vectors $\mathbf{x}_t$ only}
The easiest way to constraint is to normalize or standardize each row of \((\mathbf{x}_{1},\ldots,\mathbf{x}_{T})\). Let \(\mathbf{X} = \left( \mathbf{x}_{1},\ldots,\mathbf{x}_{T} \right) = \begin{pmatrix}
	\mathbf{X}_{(1)}^{'} \\
	\vdots \\
	\mathbf{X}_{(Jp)}^{'} \\
\end{pmatrix}\), where \(\mathbf{x}_{(k)} \in \mathbb{R}^{T}\) is the k-th row vector. Then,
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	Normalization: \(\mathbf{x}_{(k)}^* = \frac{\mathbf{x}_{(k)}-\min \mathbf{x}_{(k)}}{\max \mathbf{x}_{(k)} - \min \mathbf{x}_{(k)}}\)
	\item
	Standardization: \(\mathbf{x}_{(k)}^* = \frac{\mathbf{x}_{(k)} - \bar{\mathbf{x}}_{(k)}}{S(\mathbf{x}_{(k)})}\)
\end{enumerate}
The purpose of this constraint is to let $\bm{d}$ capture the translation and $\bm{C}$ capture the scaling.

After normalization/ standardization, the solution should be unique.

\subsubsection{Joint constraint on $\mathbf{x}_t$, $\mathbf{C}$ and linear dynamics}

\textbf{This is perfect when cluster is known} (more robust than constraints on $\bm{x}_t$ only), but when doing clusters, this is not very useful... Since the constraint depends on the number of elements in the cluster, but the number of elements will change when doing clustering.

See details in the appendix.


\subsection{Constraint Implementation}
The constraint for normalization and standardization are fairly easy: just do transformation of the MCMC sample in each iteration.

Normalization and standardization are both linear transformation, so after some algebra, we can even write the full conditional distribution explicitly.

For example, let's write out the linear transformation for centering. Denote the k-th row mean as
\({\overline{\mathbf{x}}}_{(k)} = \frac{1}{T}\mathbf{x}_{(k)}^{'}\mathbf{1}_{T}\). The unconstraint full conditional distribution of \(vec(\mathbf{X})\) is derived in previous section. After
normalization in each iteration, we are actually sampling from
\(P(vec(\mathbf{X}) - \mathbf{1}_{T}\bigotimes\begin{pmatrix}
	{\overline{\mathbf{x}}}_{(1)} \\
	\vdots \\
	{\overline{\mathbf{x}}}_{(Jp)} \\
\end{pmatrix}|\mathbf{Y},\ldots)\). Notice that,
\[\mathbf{1}_{T}\bigotimes\begin{pmatrix}
	{\overline{\mathbf{x}}}_{(1)} \\
	\vdots \\
	{\overline{\mathbf{x}}}_{(Jp)} \\
\end{pmatrix} = \mathbf{1}_{T}\bigotimes\frac{1}{T}\mathbf{X1}_{T} = \frac{1}{T}vec\left( \mathbf{1X}_{T}\mathbf{1}_{T}^{'} \right) = \frac{\mathbf{J}_{T}\bigotimes \mathbf{I}_{Jp}}{T}vec(\mathbf{X})\]
, then we are just sampling from full conditional distribution of
\((\mathbf{I}_{JpT} - \frac{\mathbf{J}_{T}\bigotimes \mathbf{I}_{Jp}}{T})vec(\mathbf{X})\)

To help the convergence, we may further add constraint on linear dynamics. Most of them can be fairly implemented, by diagonal $\mathbf{A}$ with full $\mathbf{Q}$ need some algebra. See details in the appendix.

\subsection{Simulation}
Here, I ran both normalization and standardization for 10,000 iterations. Although pure normalization or standardization should be enough to ensure convergence, further constraints in linear dynamics may help convergence.
Several constraints in linear dynamics (i.e. $\bm{A}$ and $\bm{Q}$) are also implemented:
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	No constraints in $\bm{A}$ or $\bm{Q}$
	\item
	No constraints in $\bm{A}$, but $\bm{Q}$ is block-diagonal.
	\item
	No constraints in $\bm{A}$, but $\bm{Q}$ is diagonal.
	\item
	$\bm{A}$ is diagonal, but no constraints in $\bm{Q}$
	(See details in appendix).
	\item
	Both $\bm{A}$ and $\bm{Q}$ are diagonal.
\end{enumerate}

By limited experiments, it seems when $\bm{A}$ is diagonal, the convergence \& mixing is best. If we put no constraint on $\bm{A}$, then putting no constraint on $\bm{Q}$ is the best. Here I show the 2-norm/ Frobenius norm of \(\left\{ \mathbf{d},\mathbf{C},\mathbf{X},\mathbf{b},\mathbf{A},\mathbf{Q} \right\}\) for 4 constraints combos:
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	Normalization + both $\bm{A}$ and $\bm{Q}$ are diagonal.
	\item
	Normalization + $\bm{A}$ diagonal but $\bm{Q}$ full.
	\item
	Standardization + both $\bm{A}$ and $\bm{Q}$ are diagonal.
	\item
	Standardization + $\bm{A}$ diagonal but $\bm{Q}$ full.
\end{enumerate}


\begin{figure}[h!]
	\makebox[\linewidth][c]{%
		\centering
		\subfigure[normalization + $\bm{A}$ and $\bm{Q}$ diagonal]{\label{fig:a}\includegraphics[width=0.75\textwidth]{norm_AQ_diag_dCX_trace_full.png}}%
		\subfigure[normalization + $\bm{A}$ diagonal and $\bm{Q}$ full]{\label{fig:b}\includegraphics[width=0.75\textwidth]{norm_A_diag_dCX_trace_full.png}}%
	}
	\caption{normalization, full trace of $\bm{d}$, $\bm{C}$ and $\bm{X}$}
	\label{full trace, normal, obs}
\end{figure}

\begin{figure}[h!]
	\makebox[\linewidth][c]{%
		\centering
		\subfigure[standardization + $\bm{A}$ and $\bm{Q}$ diagonal]{\label{fig:a}\includegraphics[width=0.75\textwidth]{std_AQ_diag_dCX_trace_full.png}}%
		\subfigure[standardization + $\bm{A}$ diagonal and $\bm{Q}$ full]{\label{fig:b}\includegraphics[width=0.75\textwidth]{std_A_diag_dCX_trace_full.png}}%
	}
	\caption{standardization, full trace of $\bm{d}$, $\bm{C}$ and $\bm{X}$}
	\label{full trace, std, obs}
\end{figure}

\begin{figure}[h!]
	\makebox[\linewidth][c]{%
		\centering
		\subfigure[normalization + $\bm{A}$ and $\bm{Q}$ diagonal]{\label{fig:a}\includegraphics[width=0.75\textwidth]{norm_AQ_diag_bAQ_trace_full.png}}%
		\subfigure[normalization + $\bm{A}$ and $\bm{Q}$ full]{\label{fig:b}\includegraphics[width=0.75\textwidth]{norm_A_diag_bAQ_trace_full.png}}%
	}
	\caption{normalization, full trace of $\bm{b}$, $\bm{A}$ diagonal and $\bm{Q}$}
	\label{full trace, normal, dynamics}
\end{figure}

\begin{figure}[h!]
	\makebox[\linewidth][c]{%
		\centering
		\subfigure[standardization + $\bm{A}$ and $\bm{Q}$ diagonal]{\label{fig:a}\includegraphics[width=0.75\textwidth]{std_AQ_diag_bAQ_trace_full.png}}%
		\subfigure[standardization + $\bm{A}$ and $\bm{Q}$ full]{\label{fig:b}\includegraphics[width=0.75\textwidth]{std_A_diag_bAQ_trace_full.png}}%
	}
	\caption{standardization, full trace of $\bm{b}$, $\bm{A}$ diagonal and $\bm{Q}$}
	\label{full trace, std, dynamics}
\end{figure}







\subsection{Comments on Previous Models}
When fitting the LDS or factor analysis (e.g. GPFA) model, I may only believe the temporal pattern or the change of the pattern. The pattern itself at a specific time point doesn't make a lot of sense, because of the affine transformation invariance.

In previous work, they fit models by EM, variational Bayes or some other MAP/ MLE method. \textbf{This is quite dangerous} if they don't check the uniqueness (Thanks to the gold standard MCMC, otherwise I will never check this issue...). 

In the PLDS code that Ian shared me before, the default model has no constraint. They fit it by EM, and use ELBO as the convergence criteria. Although the parameters are not unique, the likelihood keeps the same, using ELBO is justified. However, since there's no randomness in EM, it's impossible to detect the non-uniqueness. Actually, I think their results are even not local optima, but just a single point in the optimal surface (I guess if they use something like stochastic EM, they will find the issue).  

The PLDS also provides some constraint options, but some of them are still not enough to ensure unique solution. BTW, this might be another reason they choose to use EM, since adding constraint in EM is trivial: just do unconstrained EM in each iteration and project the value to the constraint space (i.e. add constraint) at the end of each iteration. But this doesn't work for MCMC, since we are sampling the distribution but not just the single optimal value.

\textbf{I guess similar problems may happen in other LDS/ GPFA research.}

Even if they make enough constraints to ensure uniqueness, these research should be careful about the pattern they found. For example, in \href{https://papers.nips.cc/paper/2020/hash/aa1f5f73327ba40d47ebce155e785aaf-Abstract.html}{Joshua et al., 2020}, the changes/ different states of interaction make sense to me, but I'm quite skeptical about their results in the dynamic matrix $\mathbf{A}$ itself. Maybe because of block-diagonal structure in loading $\mathbf{C}$, the interaction between population make sense somewhat. But at least the relationship within population doesn't make any sense, because of transformation invariance.

\clearpage


\section{TODO}
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
%	Think about MV-(Poisson)-GLM. Check \href{https://www.tandfonline.com/doi/full/10.1080/03610926.2012.743565?journalCode=lsta20}{ref1} and \href{https://www.tandfonline.com/doi/full/10.1080/02664763.2021.1877637?src=recsys}{ref2}.
	Improve the NUTS.
	\item
	Debug the clustering.
	\item
	Check and clear typo.
%	\item
%	After resolving (1), see if the MCMC is still trapped in local optima sometimes. Now, the chain seems will get stuck in local mode. Maybe updating the loading as a whole will remedy the problem a bit.
	\item
	Implement the mixture of finite mixture (MFM) by Jeff Miller.
	\item
	Find a more efficient way to generate new cluster parameters, otherwise the newly generated ones will always be rejected.
%	\item
%	improve DPMM. In current brute force implementation, the number of potential clusters can even go beyond number of neurons (\(N\)). There are several improvements, e.g. \href{https://link.springer.com/article/10.1007/s11222-009-9150-y}{Kalli et al., 2011}, \href{http://proceedings.mlr.press/v37/gea15.html}{Ge et al., 2015} and \href{https://link.springer.com/article/10.1007/s11222-014-9471-3}{Hastie et al, 2015}. Check them later.
	\item
	After all of them are resolved, switch to GP version
	
\end{enumerate}


\clearpage
\section{Appendix}

\subsection{Joint constraint on $\mathbf{x}_t$, $\mathbf{C}$ and linear dynamics}
For $\mathbf{x}_t$, we can row center it as previous. But instead of doing scaling on $\bm{x}_t$, we can put the scaling to the loading $\mathbf{C}$.

To mimic PCA, one natural way is to make the loading orthogonal. Here, I simply make \(\mathbf{C}'^{(j)}\mathbf{C}^{(j)} = \mathbf{I}_{p}\).

\textbf{However, this is not enough!} Consider \(\mathbf{x}_{t}^{*} = \mathbf{Ux}_{t}\), to
preserve the orthogonality and block diagonal on \(\mathbf{C}\), \(\mathbf{U}\) must be
block diagonal and each diagonal block \(\mathbf{U}^{(j)}\) is an orthogonal matrix. More explicitly,

\[\begin{pmatrix}
	\mathbf{C}^{(1)} & \cdots & \mathbf{0} \\
	\vdots & \ddots & \vdots \\
	\mathbf{0} & \cdots & \mathbf{C}^{(J)} \\
\end{pmatrix}\begin{pmatrix}
	\mathbf{U}^{'(1)} & \cdots & \mathbf{0} \\
	\vdots & \ddots & \vdots \\
	\mathbf{0} & \cdots & \mathbf{U}^{'(J)} \\
\end{pmatrix} = \begin{pmatrix}
	\mathbf{C}^{(1)}\mathbf{U}^{'(J)} & \cdots & \mathbf{0} \\
	\vdots & \ddots & \vdots \\
	\mathbf{0} & \cdots & \mathbf{C}^{(1)}\mathbf{U}^{'(J)} \\
\end{pmatrix}\]

This transformation will also preserve the Frobenius norm of \(\mathbf{x}_{t}\),
but the element value is not preserved. To ensure the unique solution,
we need to add more constraints on linear dynamics, i.e. \(\mathbf{A}\) or/and
\(\mathbf{Q}\). Simply making \(\mathbf{A}\) or \(\mathbf{Q}\) be block diagonal or even diagonal
with same absolute value (e.g., \(\mathbf{I}_{Jp}\)) is still not enough. One solution is to force at least one of \(\mathbf{A}\) and \(\mathbf{Q}\) be diagonal.

Take \(\mathbf{A}\) for example. Assume the true \(\mathbf{A}\) is diagonal and true
\(\mathbf{C}^{\{ j\}}\) has orthogonal column. As shown previous, the linear transformation \(\mathbf{U}\) must be block diagonal and each diagonal block
\(\mathbf{U}^{(j)}\) is an orthogonal matrix. Further, to preserve the diagonal
structure of \(\mathbf{A}\) (the elements on the diagonal are generally not the
same), we need \(\mathbf{U}^{(j)}\) to be diagonal. These two constraints force
\(\mathbf{U}\) be diagonal with diagonal elements be \(\pm 1\). So the solution
is unique up to sign. The same rationale for constraint on \(\mathbf{Q}\).

In summary, we need add three constraints if do things jointly: (1) translation of \(\mathbf{x}_{t}\),
(2) orthogonality of loading and (3) diagonality of linear dynamics. In the following implementation, I tried 3 sets of constraints:

\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	row mean in \((\mathbf{x}_{1},\ldots,\mathbf{x}_{T})\) is 0,
	\(\mathbf{C}'^{(j)}\mathbf{C}^{(j)} = \mathbf{I}_{p}\), \(\mathbf{A}\) is full while \(\mathbf{Q}\) is
	diagonal
	\item
	row mean in \((\mathbf{x}_{1},\ldots,\mathbf{x}_{T})\) is 0,
	\(\mathbf{C}'^{(j)}\mathbf{C}^{(j)} = \mathbf{I}_{p}\), \(\mathbf{Q}\) is full while \(\mathbf{A}\) is
	diagonal
	\item
	row mean in \((\mathbf{x}_{1},\ldots,\mathbf{x}_{T})\) is 0,
	\(\mathbf{C}'^{(j)}\mathbf{C}^{(j)} = \mathbf{I}_{p}\), both \(\mathbf{A}\) and \(\mathbf{Q}\) are diagonal
\end{enumerate}

Constraint (1) and (2) allows interaction between clusters.

Although, constraint (1) (diagonal $\mathbf{Q}$) is valid in theory, it doesn't work well in practice. Since elements $\mathbf{Q}$ is usually very small (~$10^{-3}$ to $10^{-5}$).
Therefore, I will discard this constraint later.

\subsection{sample the orthogonal loading}
The constraint on orthogonality is kind of tricky. Here, I just transformed \(\mathbf{C}^{(j)}\) be
\(\mathbf{C}^{(j)} = \mathbf{K}^{(j)}\left( \mathbf{K}^{'(j)}\mathbf{K}^{(j)} \right)^{- 1/2}\). Therefore,
we can just sample \(\mathbf{K}^{(j)}\) without any constrained. Since
\(\mathbf{K}^{(j)}\) has the same structure as \(\mathbf{C}^{(j)}\), we can put some
hierarchical prior to model \(\mathbf{C}^{(j)}\). Specially, when
\(\mathbf{K}^{(j)} \sim MN_{n,p}(\mathbf{0},\bm{\Sigma}^{(j)},\mathbf{I})\) (\(MN_{n,p}\) means a matrix
normal distribution), \(\mathbf{C}^{(j)} \sim MACG(\bm{\Sigma}^{(j)})\).
\(MACG(\bm{\Sigma})\) means the matrix angular central Gaussian distribution,
with density
\(f_{\mathbf{X}}(\mathbf{X}) = |\bm{\Sigma}\left. \  \right|^{- p/2}{|\mathbf{X}'\Sigma^{- 1}\mathbf{X}|}^{- n/2}\).
When \(\bm{\Sigma}^{(j)} = \mathbf{I}_{n}\) or \(n = p\), \(\mathbf{C}^{(j)}\) is just
uniformly distributed on the Stiefel manifold
\(S(p,n) = \left\{ \mathbf{C} \in \mathbb{R}^{n \times p} \right|\mathbf{C}'\mathbf{C} = \mathbf{I}_{p}\}\).

Since now there's no closed form for full conditional, I just sample it by no U-turn sampler (NUTS).

\subsection{diagonal $\mathbf{A}$ and full $\mathbf{Q}$}
Denote the diagonal \(\mathbf{A}\) as \(\mathbf{A} = diag(a_{1},\ldots,a_{Jp})\). Then,

\[\mathbf{Y}_{BA} = \begin{pmatrix}
	\mathbf{x}_{2}^{'} \\
	\mathbf{x}_{3}^{'} \\
	\vdots \\
	\mathbf{x}_{T}^{'} \\
\end{pmatrix} = \begin{pmatrix}
	1 & x_{1,1} & \cdots & x_{1,Jp} \\
	1 & x_{2,1} & \cdots & x_{2,Jp} \\
	\vdots & \vdots & \vdots & \vdots \\
	1 & x_{T - 1,1} & \cdots & x_{T - 1,Jp} \\
\end{pmatrix}\begin{pmatrix}
	b_{1} & \cdots & b_{Jp} \\
	a_{1} & \cdots & 0 \\
	\vdots & \ddots & \vdots \\
	0 & \cdots & a_{Jp} \\
\end{pmatrix} + \mathbf{E} = \mathbf{X}_{BA}\mathbf{F}_{BA} + \mathbf{E}\]

, where
\(\mathbf{E} = \left( \bm{\epsilon}_{1},\ldots,\bm{\epsilon}_{T - 1} \right)^{'}\)
and \(\bm{\epsilon}_{t} \sim N(\mathbf{0},\mathbf{Q})\). Notice that
\(\mathbf{x}_{t} = (x_{t, 1}, \cdots x_{t, Jp})'\).

Equivalently, we can rewrite the model as:

\[\mathbf{Y}_{ba} = vec\left(\mathbf{Y}_{BA} \right) = \begin{pmatrix}
	1 & x_{1,1} & \cdots & 0 & 0 \\
	\vdots & \vdots & \vdots & \vdots & \vdots \\
	1 & x_{T - 1,1} & \cdots & 0 & 0 \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	0 & 0 & \cdots & 1 & x_{1,Jp} \\
	\vdots & \vdots & \vdots & \vdots & \vdots \\
	0 & 0 & \cdots & 1 & x_{T - 1,Jp} \\
\end{pmatrix}\begin{pmatrix}
	b_{1} \\
	a_{1} \\
	\vdots \\
	b_{Jp} \\
	a_{Jp} \\
\end{pmatrix} + vec(\mathbf{E}) = \mathbf{X}_{ba}\mathbf{f}_{ba} + vec(\mathbf{E})\]

, where
\(Cov\left(vec(\mathbf{E}) \right) = \mathbf{Q}\bigotimes \mathbf{I}_{T - 1}\).

If we use the prior:
\begin{align*}
	\mathbf{Q} &\sim W^{- 1}(\Psi_{Q_{0}},\nu_{Q_{0}})\\
	\mathbf{f}_{ba}|\mathbf{Q} &\sim N_{2Jp}(\mathbf{1}_{Jp}\bigotimes \mathbf{f}_{ba,0},\ \mathbf{Q}\bigotimes\bm{\Lambda}_{ba,0}^{- 1})
\end{align*}

, where \(\mathbf{f}_{ba,0} = (0, 1)'\) and \(\bm{\Lambda}_{ba,0} = \mathbf{I}_{2}\). Then the
corresponding prior for \(vec(\mathbf{F}_{BA})\) is:

\[vec\left( \mathbf{F}_{BA} \right)|\mathbf{Q} \sim N(vec\left( \mathbf{F}_{BA,0} \right),\ \mathbf{Q}\bigotimes\bm{\Lambda}_{BA,0}^{- 1})\]

, where \(\mathbf{F}_{BA,0} = \begin{pmatrix}
	\mathbf{0}_{Jp}^{'} \\
	\mathbf{I}_{Jp} \\
\end{pmatrix}\) and \(\bm{\Lambda}_{BA,0} = \mathbf{I}_{Jp + 1}\).

Then the posteriors are:
\begin{align*}
	\mathbf{Q}|\mathbf{Y},\ldots &\sim W^{- 1}(\Psi_{Q},\nu_{Q})\\
	\mathbf{f}_{ba}|\mathbf{Y},\mathbf{Q},\ldots &\sim N({\widehat{\mathbf{f}}}_{ba},\ (\mathbf{Q}\bigotimes \mathbf{I}_{2})\Lambda_{ba}^{- 1})
\end{align*}

, with parameters be:
\begin{align*}
	\bm{\Lambda}_{ba} &= \mathbf{X}_{ba}^{'}\mathbf{X}_{ba} + \mathbf{I}_{Jp}\bigotimes\bm{\Lambda}_{ba,0}\\
	{\widehat{\mathbf{f}}}_{ba} &= \bm{\Lambda}_{ba}^{- 1}\left( \mathbf{X}_{ba}^{'}\mathbf{Y}_{ba} + \ \mathbf{1}_{Jp}\bigotimes\bm{\Lambda}_{ba,0}\mathbf{f}_{ba,0} \right)
\end{align*}

The corresponding estimate for \(\mathbf{F}_{BA}\) is denoted as
\({\widehat{\mathbf{F}}}_{BA}\), which is just a transformation of \({\widehat{\mathbf{f}}}_{ba}\).
\begin{align*}
	\Psi_{Q} &= \Psi_{Q0} + \left( \mathbf{Y}_{BA} - \mathbf{X}_{BA}{\widehat{\mathbf{F}}}_{BA} \right)^{'}\left( \mathbf{Y}_{BA} - \mathbf{X}_{BA}{\widehat{\mathbf{F}}}_{BA} \right) + \left( {\widehat{\mathbf{F}}}_{BA} - \mathbf{F}_{BA,0} \right)^{'}\Lambda_{BA,0}^{- 1}\left( {\widehat{\mathbf{F}}}_{BA} - \mathbf{F}_{BA,0} \right)\\
	\nu_{Q} &= \nu_{Q0} + T - 1
\end{align*}

\section{Simulation on joint constraint}
The simulation example is as before, and all the code can be found in \href{https://github.com/weigcdsb/state-space-clustering/tree/main/LDS/blkDiag}{this folder}.

Since now we constrain the loading has orthogonal column for each cluster, just showing the Frobenius norm of $\mathbf{x}_t$ is not enough to check convergence. Here, I further check the value of $\mathbf{x}_{[T/2]}$.

For \(\mathbf{C}^{(j)} = \mathbf{K}^{(j)}\left( \mathbf{K}^{'(j)}\mathbf{K}^{(j)} \right)^{- 1/2}\), I just put a simple hierarchical prior on $\mathbf{K}^{(j)}$ such that each element of $\mathbf{K}^{(j)}$ is i.i.d. $N(\mu_{k}^{(j)}, (\sigma^{(j)}_{k})^2)$. Well, maybe we can put some more careful priors later to reflect the covariance among neurons.

Basically, I ran both MCMCs for 10,000 iterations. Here, I show 6 sets of plots:
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	Full trace of  2-norm/ Frobenius norm in $\bm{d}$, $\bm{C}$ and $\mathbf{X} = (\mathbf{x}_{1},\ldots,\mathbf{x}_{T})$
	\item
	Full trace of 2-norm/ Frobenius norm in $\mathbf{b}$, $\mathbf{A}$ and $\mathbf{Q}$ 
	\item
	partial trace plot of (1) from iteration 1 to 1000
	\item
	partial trace plot of (2) from iteration 1 to 1000
	\item
	average mean firing rate from iteration 5000 to 10,000.
	\item
	average $\mathbf{X}$ from iteration 5000 to 10,000.
\end{enumerate}

\subsubsection{\(\mathbf{Q}\) is full while \(\mathbf{A}\) is diagonal}
\begin{figure}[h!]
	\makebox[\linewidth][c]{%
		\centering
		\subfigure[tracd of $\mathbf{X}$]{\label{fig:a}\includegraphics[width=0.75\textwidth]{CCI_AQ_diag_dCX_trace_full.png}}%
		\subfigure[trace of loading and dynamics]{\label{fig:b}\includegraphics[width=0.75\textwidth]{CCI_AQ_diag_bAQ_trace_full.png}}%
	}
	\caption{\(\mathbf{Q}\) is full while \(\mathbf{A}\) is diagonal, full trace}
	\label{full trace, diag A}
\end{figure}

\begin{figure}[h!]
	\makebox[\linewidth][c]{%
		\centering
		\subfigure[tracd of $\mathbf{X}$]{\label{fig:a}\includegraphics[width=0.75\textwidth]{CCI_AQ_diag_dCX_trace_part.png}}%
		\subfigure[trace of loading and dynamics]{\label{fig:b}\includegraphics[width=0.75\textwidth]{CCI_AQ_diag_bAQ_trace_part.png}}%
	}
	\caption{\(\mathbf{Q}\) is full while \(\mathbf{A}\) is diagonal, partial trace}
	\label{partial trace, diag A}
\end{figure}

\begin{figure}[h!]
	\makebox[\linewidth][c]{%
		\centering
		\subfigure[mean firing rate]{\label{fig:a}\includegraphics[width=0.75\textwidth]{FR_diagA.png}}%
		\subfigure[latent vectors]{\label{fig:b}\includegraphics[width=0.75\textwidth]{latent_diagA.png}}%
	}
	\caption{\(\mathbf{Q}\) is full while \(\mathbf{A}\) is diagonal, mean FR and $\mathbf{x}_t$}
	\label{FR and X, diag A}
\end{figure}
\clearpage

\subsubsection{both \(\mathbf{A}\) and \(\mathbf{Q}\) are diagonal}
\begin{figure}[h!]
	\makebox[\linewidth][c]{%
		\centering
		\subfigure[tracd of $\mathbf{X}$]{\label{fig:a}\includegraphics[width=0.75\textwidth]{CCI_A_diag_dCX_trace_full.png}}%
		\subfigure[trace of loading and dynamics]{\label{fig:b}\includegraphics[width=0.75\textwidth]{CCI_A_diag_bAQ_trace_full.png}}%
	}
	\caption{both \(\mathbf{A}\) and \(\mathbf{Q}\) are diagonal, full trace}
	\label{full trace, diag AQ}
\end{figure}

\begin{figure}[h!]
	\makebox[\linewidth][c]{%
		\centering
		\subfigure[tracd of $\mathbf{X}$]{\label{fig:a}\includegraphics[width=0.75\textwidth]{CCI_A_diag_dCX_trace_part.png}}%
		\subfigure[trace of loading and dynamics]{\label{fig:b}\includegraphics[width=0.75\textwidth]{CCI_A_diag_bAQ_trace_part.png}}%
	}
	\caption{both \(\mathbf{A}\) and \(\mathbf{Q}\) are diagonal, partial trace}
	\label{partial trace, diag AQ}
\end{figure}

\begin{figure}[h!]
	\makebox[\linewidth][c]{%
		\centering
		\subfigure[mean firing rate]{\label{fig:a}\includegraphics[width=0.75\textwidth]{FR_diagAQ.png}}%
		\subfigure[latent vectors]{\label{fig:b}\includegraphics[width=0.75\textwidth]{latent_diagAQ.png}}%
	}
	\caption{both \(\mathbf{A}\) and \(\mathbf{Q}\) are diagonal, mean FR and $\mathbf{x}_t$}
	\label{FR and X, diag AQ}
\end{figure}

We can see that either is fine, and the convergence is achieved at nearly 20 to 30 iterations.

\textbf{In summary, to ensure convergence, we can put the following two constraints:}
\begin{enumerate}
	\def\labelenumi{(\arabic{enumi})}
	\item
	row mean in \((\mathbf{x}_{1},\ldots,\mathbf{x}_{T})\) is 0,
	\(\mathbf{C}'^{(j)}\mathbf{C}^{(j)} = \mathbf{I}_{p}\), \(\mathbf{Q}\) is full while \(\mathbf{A}\) is
	diagonal
	\item
	row mean in \((\mathbf{x}_{1},\ldots,\mathbf{x}_{T})\) is 0,
	\(\mathbf{C}'^{(j)}\mathbf{C}^{(j)} = \mathbf{I}_{p}\), both \(\mathbf{A}\) and \(\mathbf{Q}\) are diagonal
\end{enumerate}

If we really care about the interactions between clusters, just use constraint (1). Otherwise, either is fine.


\end{document}






